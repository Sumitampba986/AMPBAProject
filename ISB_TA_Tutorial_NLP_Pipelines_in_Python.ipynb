{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qp-kum7jK8y"
   },
   "source": [
    "# Introduction to NLP Annotation Pipelines in Python\n",
    "\n",
    "**Burt Monroe (Penn State University)**\n",
    "\n",
    "**Material for \"Text as Data\" at Penn State and \"Advanced Text as Data\" at Essex Summer School in Social Science Data Analysis**\n",
    "\n",
    "This is an overview/refresher/primer on basic NLP tasks like segmentation, tokenization, stemming, lemmatization, part-of-speech tagging, named entity recognition, and dependency parsing with the most common Python libraries. These are generally tasks that divide the text into pieces (e.g., sentences, tokens, noun phrases), provide alternative forms for thoses pieces (e.g., lemmatization, stemming), and provide annotations/labels for those pieces (e.g., part-of-speech, position in a dependency parse tree).  There is, of course, more detail available in the documentation of each library. There is a parallel notebook for similar tasks in R here: https://colab.research.google.com/drive/15UcuXNYuhR9wuHbKp4J1HUsOB6GOX6oV?usp=sharing.\n",
    "\n",
    "The main NLP tasks in the main packages are all organized around the concept of an annotation \"pipeline\" based on a given \"language model.\" Putting aside for the moment the ability to train your own model, in essence you download/install/load a given model which can vary in the language to which it applies, how large it is (larger models have been trained on more data, but take up more space and computational resources/time), the objectives against which it was trained, and the tasks to which it can be applied. You then pass the model and the input text to the software's annotation pipeline -- in many cases with some control over particular components and order -- and receive an output object with the annotated text.\n",
    "\n",
    "For example, the pipeline for Stanford CoreNLP is depicted below. The text is first tokenized, then split into sentences, then tokens are tagged with respect to parts of speech, then the tokens are lemmatized, then the named entity recognizer is applied, and finally the dependency parser is applied. The output is an object from which all of those annotations can be accessed.\n",
    "\n",
    "![CoreNLP Pipeline (Source: https://stanfordnlp.github.io/CoreNLP/index.html)](https://drive.google.com/uc?id=14COzmOsaDzzWuQsziEF__73Q2cgaGPq7)\n",
    "\n",
    "We will focus here on spaCy, Stanza/coreNLP, NLTK, and Flair. (The companion R notebook discusses spaCy, UDPipe, openNLP, coreNLP, as well as related capabilities in \"text-as-data\" packages quanteda, tm, and tidytext). There are other major NLP libraries, like AllenNLP and SparkNLP, that can be used for these tasks but are probably overkill; they are more likely to be helpful for higher level natural language understanding tasks.\n",
    "\n",
    "There are some tasks -- like stemming -- where the choice of library has very little impact on the output. There are others -- like tokenization -- that vary as much or more by different settings and choices made by the analyst, than they do by library.\n",
    "Some tasks -- like named entity recognition -- are very domain-dependent. On these, the performance of the different NLP pipelines out of the box varies wildly and can dramatically influence by the interaction of a particular dataset with, for example, a tokenizer choice. See, for example, this comparison of NER by spaCy (big and small models), coreNLP, and Flair, on data from the Federal Register and Tweets by American politicians. (Or this comparison of spaCy, NLTK, StanfordNLP, OpenNLP, and Gate: http://sylvainkubler.fr/wp-content/themes/biopic/images/publications/documents/SNAMS_2019.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P_XFo6RvTSy"
   },
   "source": [
    "## NLP Glossary\n",
    "\n",
    "In this section, I repeat / gather  some general information from the slides and readings about terminology and concepts important to understanding the functionality of the NLP software packages discussed here.\n",
    "\n",
    "A **corpus** is a digitized collection of text. \"Corpus\" is pluralized as **corpora**.\n",
    "\n",
    "A **type** is an element of the vocabulary. (Sometimes the number of distinct types in the vocabulary is referred to as $V$; sometimes $V$ refers to the vocabulary as a `set`, in which case the number of distinct types is $|V|$.) A **token** is an instance of a type in running text. (We typically refer to the number of tokens in a corpus as $N$.)\n",
    "\n",
    "**Tokenization** is the division of text into tokens. This may mean \"words\" split on spaces and punctuation, but even that is problematic. We may wish in some cases not to throw out punctuation (e.g.,\"!!!\"or \"$\" or `#blessed`), not to split on punctuation (e.g., `D.C.` or `burmonroe@psu.edu`), or not to split on spaces (`District of Columbia` or `Supreme Court`). Some languages have compound words that we may wish to separate into smaller units of meaning -- e.g., the Bundestag parliamentary record contains this gem:  `Verkehrswegeplanungsbeschleunigungsgesetzen` (Laws for the acceleration of the planning of traffic routes). Some languages don't have spaces (e.g., Chinese, Thai). In Chinese, a common approach is to treat individual characters (hanzi) as tokens. The presence/count/weighted count of tokens will often serve as the set of **features** used to represent documents in statistical / machine learning models. There are many many tokenizers for something like English, and most text analysis packages have a default that might not be ideal for your particular purpose.\n",
    "\n",
    "**Segmentation** is a term that can include standard tokenization -- \"Word segmentation\" may be used to refer to separating words, especially in languages like Japanese or Thai, into meaningful word-like tokens -- but it is also applied to processes like splitting a text into paragraphs or sentences. Sentence segmentation is important but not as trivial as it might seem. To take the most obvious example, a \"period\" usually separates sentences, but not always, e.g. `Dr. Jane R. Smith, Ph.D., lives 3.5 miles from D.C.` Informal text from social media does not follow conventional punctuation standards. People don't speak with punctuation, so transcribed speech is ambiguous. Candidate Donald Trump gave a speech in 2016 where the press transcript included `\"Very small crowds, you know it, they know it, everybody knows it\"` (one \"sentence\") as well as `\"Highly respected man. Four star general.\"` (two \"sentences\").\n",
    "\n",
    "A modern approach to tokenization, used in the BERT model we will discuss at length, is **wordpiece tokenization**, which learns -- through a variant of a process called **byte-pair encoding (BPE)** -- meaningful subword pieces -- or **morphemes** to use as tokens. A good example in English is `-est`. This is also used to address the **out-of-vocabulary (OOV)** problem that can arise when encountering new words (or unintentionally new words, like misspellings), by dividing words into recognizable subwords, and in the extreme to individual characters, to use as tokens.\n",
    "\n",
    "You will frequently see feature sets defined on the basis of **ngrams** -- sequences of $n$ tokens in a row -- with specific examples usually referred to as **unigrams**, **bigrams**, **trigrams**, **4-grams**, **5-grams**, and so on.\n",
    "\n",
    "You will in some circumstances see a similar concept with features based on sequences of $k$ characters referred to as **shingles** or **k-shingles**. (I have also seen this referred to as \"character ngrams\" and seen word ngrams referred to as shingles -- it's not 100% consistent.)\n",
    "\n",
    "Sometimes text features are implemented with the **hashing trick** applying a mathematical formula to map strings (tokens, shingles, etc.) to one of a large but finite set of integers, which are then used as the actual features. This has a number of computational advantages, and the OOV problem is avoided since every possible string maps to a defined feature. The tradeoff is that there may be **collisions** in which two or more different tokens/shingles map to the same integer and it may be difficult to back out interpretations of the impact of particular features.\n",
    "\n",
    "Since $|V|$ can be quite large, and feature sets based on ngrams and similar even larger, text data is high-dimensional and it is common for text-as-data analyses to involve **feature selection**, removing for example rare tokens. It is also common in some applications to remove punctuation, numbers, and/or **stopwords** -- words that play a functional or filler role that conveys little meaning (e.g., articles like `the` or `a`, prepositions like `of` or `in`, conjunctions like `and` or `or`, pronouns like `he` or `she`, etc.) or are so common that they convey little distinguishing meaning (like `Madame Speaker` in parliamentary speech). This can be problematic however, as such lists have no nonarbitrary criteria for inclusion and can exclude highly meaningful tokens for some applications (e.g., gendered pronouns can be an important indicator of ideology in political speech, exclamation points can be an important indicator of emotion in social media posts, function words can be an important indicator of authorship in stylometric analysis).\n",
    "\n",
    "Text **normalization** is the process of putting tokens / textual features in a standard form. Tasks like topic modeling and text classification depend on repeated patterns, so we want to recognize when, for example, two superficially different tokens represent the same repeated meaning. This helps, for example, avoid some of the problems induced by sparsity and high dimensionality in text data. A simple and common type of normalization is **case folding**, typically converting all upper case characters to lower case. `This` and `this` become the same thing, which is probably what we want. And it may overdo it, conflating for example `Trump` and `trump` or `US` and `us`.\n",
    "\n",
    "For many languages, an important form of normalization is **unicode normalization**. This is especially important where things like accents, diacritics, ligatures, and font variants can lead to the same word being represented differently in source texts. There is a good discussion of this in an NLP context here: https://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0. See also the official unicode documentation (https://unicode.org/reports/tr15/). In R, the stringi library command `stri_trans_nf` can be used for unicode normalization. (In Python, the library `unicodedata` is useful.)\n",
    "\n",
    "A **wordform** is a word fully inflected as it appears in running text. That last sentence contains wordforms `is`, `word`, `fully`, `inflected`, `appears`, and `running`. A **lemma** is the uninflected root of any given wordform. The corresponding lemmas for the previous list of wordforms are then `be`, `word`, `full`, `inflect`, `appear`, and `run`. **Lemmatization** is the process of identifying the lemma corresponding to any observed wordform. Lemmatization involves **morphological parsing**, identifying **stems** and **affixes** (both \"morphemes\") that combine to give any particular wordform its meaning.\n",
    "\n",
    "An approximate form of lemmatization is **stemming**, which applies a computationally efficient, but mechanistic, set of rules in sequence to identify word stems. The popular Porter stemmer, which includes special rules for \"to be,\" reduces all of the wordforms in the example above to the correct lemma except `fully` which it reduces to `fulli` (following the same rule that would reduce `pully` and `pullies` to `pulli`). This is mostly fine, missing the common root of `full` and `fully`, but not making things worse than before stemming, since there's nothing else that would reduce to `fulli`. There are plenty of examples, though, where it will conflate words it shouldn't (e.g., `organization` & `organs` -> `organ`) or fail to lump words it probably should (e.g., `noise` -> `nois` and `noisy` -> `noisi`). And nothing we've discussed so far can separate `can` in `yes we can` from `can` in `tomato soup can`.\n",
    "\n",
    "NLP is often concerned with word- or token-level tasks, like **part-of-speech tagging**, assigning a part-of-speech label to a given token. These word-level classification problems are typically referred to as **tagging** or **sequence labeling** or **annotation** tasks. Word-level tasks like part-of-speech tagging interact with sentence-level tasks for identifying grammatical / syntactical structure that ties words together, which are typically referred to as **parsing**. (You'll see \"parser\" used to describe tools that interpret non-natural language syntax as well -- such as parsers that read data from file formats like XML, JSON, CSV, or parsers that turn R or Python scripts into executable instructions for a computer.) There are also labeling tasks that reside between token-level and sentence-level. One of the most common/important is **named entity recognition (NER)**, which seeks to label tokens or multiword phrases that act as proper nouns representing, for example, people, places, and organizations.\n",
    "\n",
    "The \"ground truth\" for such tagging tasks -- the training data for tagging algorithms -- are also typically referred to in NLP as **annotations**. Corpora, and collections of corpora, that have been annotated for tagging and parsing purposes are typically referred to as **treebanks**, since they map sentences onto various kinds of \"tree\" structures. (We'll be more precise later, but think \"diagramming sentences\", if you ever went through that in elementary school). The treebank projects you will see referenced the most in this class are the Penn Treebank (the first large-scale treebank published) and Universal Dependency Treebank projects, but there are many (-- see https://en.wikipedia.org/wiki/Treebank). One of the main ways in which particular NLP libraries or models vary is in the treebank(s) on which they were trained, which differ both in annotation rules and in the types of sources from which their sentences are drawn.\n",
    "\n",
    "Returning to part-of-speech (POS) tagging specifically, the possibilities for POS tags are slightly different across different systems. I will tend to focus on the **Universal POS tags** used by the universal dependencies system, listed here: https://universaldependencies.org/u/pos/.\n",
    "\n",
    "![Universal POS Tags (Universal Dependencies - Nivre et al. 2016). Table source: Jurafsky and Martin, *SLP3*, 2021.](https://drive.google.com/uc?id=14pu694oNdoQtzBajR6rx8glY8Dg6lx5Y)\n",
    "\n",
    "Note that many datasets and software pipelines, especially for English, use the 45-category POS tagset from the Penn Treebank project:\n",
    "\n",
    "![Penn Treebank POS Tags (Marcus et al. 1993). Table source: Jurafsky and Martin, *SLP3*, 2021.](https://drive.google.com/uc?id=1_qVOaNjawHChT954A_J7_kH5XlPMyI_2)\n",
    "\n",
    "There are several types of parsing, but we will focus in this class on **dependency parsing**, and specifically the approach of the **Universal dependencies (UD)** project. Quoting the home page of the project (https://universaldependencies.org/), UD\n",
    "\n",
    "> is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. UD is an open community effort with over 300 contributors producing nearly 200 treebanks in over 100 languages.\"\n",
    "\n",
    "The introduction to the UD project (https://universaldependencies.org/introduction.html) describes it in more detail as follows:\n",
    "\n",
    "> Universal Dependencies (UD) is a project that is developing cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on an evolution of (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing language-specific extensions when necessary.\n",
    "\n",
    "> This is illustrated in the following parallel examples from English, Bulgarian, Czech and Swedish, where the main grammatical relations involving a passive verb, a nominal subject and an oblique agent are the same, but where the concrete grammatical realization varies.\n",
    "\n",
    "![Universal Dependencies example (from https://universaldependencies.org/introduction.html)](https://drive.google.com/uc?id=153UPGDPXetWfVFhLAwhH8Fo1yMjGs-4G)\n",
    "\n",
    "UD is the parsing approach used by the most widely used NLP tools discussed below, and is an important component of NLP research in many languages.\n",
    "\n",
    "It can be difficult to conceptualize how parse trees and other complex linguistic structures can be mapped onto the rectangular data structures needed for data analysis. One useful standard emerging from the UD project is **CoNLL-U format**, a standardized tab-separated tabular data format for annotations, consisting of one line per word/token (potentially multiword token), with ten fields including an id, the word's lemma, part-of-speech tags, and dependency relations according to the universal dependency parse.\n",
    "\n",
    "When we get to things like political event data, we are interested in **information extraction** problems, like identifying an event or action, and the source, target, location, and date of that action. This combines and leverages token-level tasks, sentence-level tasks, and higher level **natural language understanding** tasks to address things like **entity disambiguation / entity linking** (Is that \"Washington\" DC, the state, or the president or what?) and **coreference resolution** (Who's \"them\" in \"If you get them angry enough, the police show up.\" How about now: \"The neighbors hate loud music. If you get them angry enough, the police show up.\").\n",
    "\n",
    "\n",
    "Many text-as-data techniques that you have learned about in previous courses are focused on **bag-of-words** techniques. Features are approximately words, or n-grams, and documents are represented by the presence/absence/count of those features and then classified or topic-modeled or whatever on that basis. At their core, models for such tasks are based on the statistical associations of word/feature occurrence within textual units (e.g., a document) without regard for order. NLP tools here can be used to supplement that sort of approach (e.g., POS-tagging can be used to filter to just nouns for a topic model) or can be used to tackle more linguistically sensitive tasks like political event detection (where \"Russia invaded Ukraine\" and \"Ukraine invaded Russia\" need to be understood as having different meaning). At their core, the models for such tasks are **language models**, based on the statistical probabilities of words/etc. appearing in particular sequences or contexts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbrlPhVivW5y"
   },
   "source": [
    "## spaCy\n",
    "\n",
    "The **spaCy** package is, by some accounts, now the \"default\" standard NLP pipeline, especially in industry. Unlike its Python predecessor, NLTK, spaCy is \"opinionated\" -- it tries to provide easy, computationally efficient access to the best available model for any given task; NLTK provides many options and more direct ability for the researcher to test and modify different models. Also in contrast to NLTK, spaCy interacts nicely with modern neural / deep learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZCwgz5mx06S",
    "outputId": "fd7fb843-7668-4474-8813-1505e5c3178c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiN2LFBo4pi9"
   },
   "source": [
    "### The standard spaCy pipeline (tokens, lemmas, pos, dependencies, ner, morphology, etc.)\n",
    "\n",
    "You can load different models. This is just the \"English small\" model.\n",
    "\n",
    "Note that the model is loaded and assigned to the variable `nlp_spacy`. Now `nlp_spacy` is a *function* that says \"run this model's annotation pipeline on these string(s).\" This is very standard syntax for NLP pipelines.\n",
    "\n",
    "By default, spaCy runs *everything* supported by the given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtBkZWEIyBXi"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II76RY0H5Dp3"
   },
   "source": [
    "Take a peek at some of its tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_FLNL-syHMs",
    "outputId": "ffa22e0b-ddb9-4593-8656-9e9da9622576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup NOUN dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "annotated_doc_spacy = nlp_spacy(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in annotated_doc_spacy:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCm1nwRwyQ-t"
   },
   "source": [
    "Greater detail has been kept and the `annotated_doc_spacy` object doesn't need to be calculated again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbDZ3qEUyG9R",
    "outputId": "d42f419f-4a3b-4e04-c44a-7cafcf32f74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dep xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in annotated_doc_spacy:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVmhrSm8Llv7"
   },
   "source": [
    "It is worth noting that the `pos` attribute contains one of the 17 Universal POS tags discussed in the previous notebook, while the `tag` attribute contains the \"detailed\" POS tag.\n",
    "\n",
    "As I noted above, spaCy actually runs everything in the model pipeline, and for English, there are dozens of labels stored in the attributed of the annotated text.\n",
    "\n",
    "Available additional attributes of tokens are as follows. Many attributes that are intuitively strings (e.g., the token's lemma, its part of speech tag) are stored internally by spaCy as \"hashes\" (an integer). The attribute that provides the corresponding text will end in an underscore character.\n",
    "\n",
    "* `doc`: The parent document.\n",
    "* `lex`: The underlying lexeme.\n",
    "* `sent`: The sentence span that this token is a part of.\n",
    "* `text`:\tVerbatim text content.\n",
    "* `text_with_ws`:\tText content, with trailing space character if present.\n",
    "* `whitespace_`:\tTrailing space character if present.\n",
    "* `orth`:\tID of the verbatim text content.\n",
    "* `orth_`:\tVerbatim text content (identical to Token.text).\n",
    "* `vocab`:\tThe vocab object of the parent Doc.\n",
    "* `tensor`:\tThe token’s slice of the parent Doc’s tensor.\n",
    "* `head`:\tThe syntactic parent, or “governor”, of this token.\n",
    "* `left_edge`: The leftmost token of this token’s syntactic descendants.\n",
    "* `right_edge`:\tThe rightmost token of this token’s syntactic descendants.\n",
    "* `i`:\tThe index of the token within the parent document.\n",
    "* `ent_type`:\tNamed entity type. (integer)\n",
    "* `ent_type_`:\tNamed entity type. (string)\n",
    "* `ent_iob`:\tIOB code of named entity tag. 3 means the token begins an entity, 2 means it is outside an entity, 1 means it is inside an entity, and 0 means no entity tag is set.\n",
    "* `ent_iob_`:\tIOB code of named entity tag. “B” means the token begins an entity, “I” means it is inside an entity, “O” means it is outside an entity, and \"\" means no entity tag is set.\n",
    "* `ent_kb_id`:\tKnowledge base ID that refers to the named entity this token is a part of, if any. (integer)\n",
    "* `ent_kb_id_`:\tKnowledge base ID that refers to the named entity this token is a part of, if any. (string)\n",
    "* `ent_id`:\tID of the entity the token is an instance of, if any. Currently not used, but potentially for coreference resolution.\n",
    "* `ent_id_`:\tID of the entity the token is an instance of, if any. Currently not used, but potentially for coreference resolution.\n",
    "* `lemma`:\tBase form of the token, with no inflectional suffixes. (integer)\n",
    "* `lemma_`:\tBase form of the token, with no inflectional suffixes. (string)\n",
    "* `norm`:\tThe token’s norm, i.e. a normalized form of the token text. Can be set in the language’s tokenizer exceptions. (integer)\n",
    "* `norm_`:\tThe token’s norm, i.e. a normalized form of the token text. Can be set in the language’s tokenizer exceptions. (string)\n",
    "* `lower`:\tLowercase form of the token. (integer)\n",
    "* `lower_`:\tLowercase form of the token text. Equivalent to Token.text.lower(). (string)\n",
    "* `shape`:\tTransform of the token’s string to show orthographic features. Alphabetic characters are replaced by x or X, and numeric characters are replaced by d, and sequences of the same character are truncated after length 4. For example,\"Xxxx\"or\"dd\". (integer)\n",
    "* `shape_`:\tTransform of the token’s string to show orthographic features. Alphabetic characters are replaced by x or X, and numeric characters are replaced by d, and sequences of the same character are truncated after length 4. For example,\"Xxxx\"or\"dd\". (string)\n",
    "* `prefix`:\tHash value of a length-N substring from the start of the token. Defaults to N=1. (integer)\n",
    "* `prefix_`:\tA length-N substring from the start of the token. Defaults to N=1.\n",
    "* `suffix`:\tHash value of a length-N substring from the end of the token. Defaults to N=3. (integer)\n",
    "* `suffix_`:\tLength-N substring from the end of the token. Defaults to N=3.\n",
    "* `is_alpha`:\tDoes the token consist of alphabetic characters? Equivalent to token.text.isalpha().\n",
    "* `is_ascii`:\tDoes the token consist of ASCII characters? Equivalent to all(ord(c) < 128 for c in token.text).\n",
    "* `is_digit`:\tDoes the token consist of digits? Equivalent to token.text.isdigit().\n",
    "* `is_lower`:\tIs the token in lowercase? Equivalent to token.text.islower().\n",
    "* `is_upper`:\tIs the token in uppercase? Equivalent to token.text.isupper().\n",
    "* `is_title`:\tIs the token in titlecase? Equivalent to token.text.istitle().\n",
    "* `is_punct`:\tIs the token punctuation?\n",
    "* `is_left_punct`:\tIs the token a left punctuation mark, e.g. \"(\" ?\n",
    "* `is_right_punct`:\tIs the token a right punctuation mark, e.g. \")\" ?\n",
    "* `is_space`:\tDoes the token consist of whitespace characters? Equivalent to token.text.isspace().\n",
    "* `is_bracket`:\tIs the token a bracket?\n",
    "* `is_quote`:\tIs the token a quotation mark?\n",
    "* `is_currency`:\tIs the token a currency symbol?\n",
    "* `like_url`:\tDoes the token resemble a URL?\n",
    "* `like_num`:\tDoes the token represent a number? e.g. “10.9”, “10”, “ten”, etc.\n",
    "* `like_email`:\tDoes the token resemble an email address?\n",
    "* `is_oov`:\tIs the token out-of-vocabulary (i.e. does it not have a word vector)?\n",
    "* `is_stop`:\tIs the token part of a “stop list”?\n",
    "* `pos`:\tCoarse-grained part-of-speech from the Universal POS tag set. (integer)\n",
    "* `pos_`:\tCoarse-grained part-of-speech from the Universal POS tag set. (string)\n",
    "* `tag`:\tFine-grained part-of-speech. (integer)\n",
    "* `tag_`:\tFine-grained part-of-speech. (string)\n",
    "* `morph`:\tMorphological analysis.\n",
    "* `dep`:\tSyntactic dependency relation. (integer)\n",
    "* `dep_`:\tSyntactic dependency relation. (string)\n",
    "* `lang`:\tLanguage of the parent document’s vocabulary. (integer)\n",
    "* `lang_`:\tLanguage of the parent document’s vocabulary. (string)\n",
    "* `prob`:\tSmoothed log probability estimate of token’s word type (context-independent entry in the vocabulary).\n",
    "* `idx`:\tThe character offset of the token within the parent document.\n",
    "* `sentiment`:\tA scalar value indicating the positivity or negativity of the token.\n",
    "* `lex_id`:\tSequential ID of the token’s lexical type, used to index into tables, e.g. for word vectors.\n",
    "* `rank`:\tSequential ID of the token’s lexical type, used to index into tables, e.g. for word vectors.\n",
    "* `cluster`:\tBrown cluster ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "namRctZC5Ny6"
   },
   "source": [
    "Spacy provides a visualization tool for its dependency parse, called **displacy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "El-l61lByh41",
    "outputId": "42087155-f17b-40e2-ee0d-9d59af8f8ac1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d44360fb48a5448bb92205787983a139-0\" class=\"displacy\" width=\"1040\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,47.0 225.0,47.0 225.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,92.0 310.0,92.0 310.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310.0,139.0 L318.0,127.0 302.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,139.0 L408.0,127.0 392.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-4\" stroke-width=\"2px\" d=\"M430,137.0 C430,92.0 490.0,92.0 490.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M490.0,139.0 L498.0,127.0 482.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-5\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 585.0,47.0 585.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M585.0,139.0 L593.0,127.0 577.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-6\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670.0,139.0 L678.0,127.0 662.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-7\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,139.0 L782,127.0 798,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d44360fb48a5448bb92205787983a139-0-9\" stroke-width=\"2px\" d=\"M700,137.0 C700,2.0 950.0,2.0 950.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d44360fb48a5448bb92205787983a139-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M950.0,139.0 L958.0,127.0 942.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(annotated_doc_spacy, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE3eAmvy5VE5"
   },
   "source": [
    "As a sidebar, I note Spacy stores vocabulary strings internally as hashes, and these can be accessed directly in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UDM3po_y-q1",
    "outputId": "38d02e27-fe3d-454e-9b78-a8e5b3fbf22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6418411030699964375\n",
      "Apple\n"
     ]
    }
   ],
   "source": [
    "print(annotated_doc_spacy.vocab.strings[\"Apple\"]) # 6418411030699964375\n",
    "print(annotated_doc_spacy.vocab.strings[6418411030699964375]) # \"Apple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rmU_4ow5apv"
   },
   "source": [
    "As noted, the model here is \"en_core_web_sm\": en\" indicates the model is English, \"web\" indicates it is based on the Web Treebank, and \"sm\" indicates it is the small model, which is more compact and computationally efficient than the medium, large, or transformer-based models, but less accurate and does not come with pretrained embeddings. You can see which models are available, along with exact details of each model and performance statistics, here: https://spacy.io/models/ (As of this writing, there are pretrained models for 18 languages: Catalan, Chinese, Danish, Dutch, English, French, German, Greek, Italian, Japanese, Lithuania, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, and Spanish.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL806jdivHoe"
   },
   "source": [
    "### Named entities\n",
    "\n",
    "As we saw above, the spacy pipeline includes a named entity recognizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-cyMQOdzSnN",
    "outputId": "18f8ba52-689d-4c1a-b22e-c9d664d0ef19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in annotated_doc_spacy.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na3_TaWB5lfM"
   },
   "source": [
    "For comparison with the other NLP pipelines discussed below and in other notebooks, we can see what named entities spacy extracts from the Trump inaugural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXkO_fDuziZW"
   },
   "outputs": [],
   "source": [
    "trump = \"Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans, and people of the world: thank you.\\n\\nWe, the citizens of America, are now joined in a great national effort to rebuild our country and restore its promise for all of our people.\\n\\nTogether, we will determine the course of America and the world for many, many years to come.\\n\\nWe will face challenges. We will confront hardships. But we will get the job done.\\n\\nEvery four years, we gather on these steps to carry out the orderly and peaceful transfer of power, and we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition. They have been magnificent. Thank you.\\n\\nToday's ceremony, however, has very special meaning. Because today we are not merely transferring power from one Administration to another, or from one party to another - but we are transferring power from Washington DC and giving it back to you, the people.\\n\\nFor too long, a small group in our nation's Capital has reaped the rewards of government while the people have borne the cost.\\n\\nWashington flourished - but the people did not share in its wealth.\\n\\nPoliticians prospered - but the jobs left, and the factories closed.\\n\\nThe establishment protected itself, but not the citizens of our country.\\n\\nTheir victories have not been your victories; their triumphs have not been your triumphs; and while they celebrated in our nation's capital, there was little to celebrate for struggling families all across our land.\\n\\nThat all changes - starting right here, and right now, because this moment is your moment: it belongs to you.\\n\\nIt belongs to everyone gathered here today and everyone watching all across America.\\n\\nThis is your day. This is your celebration.\\n\\nAnd this, the United States of America, is your country.\\n\\nWhat truly matters is not which party controls our government, but whether our government is controlled by the people.\\n\\nJanuary 20, 2017, will be remembered as the day the people became the rulers of this nation again.\\n\\nThe forgotten men and women of our country will be forgotten no longer.\\n\\nEveryone is listening to you now.\\n\\nYou came by the tens of millions to become part of a historic movement the likes of which the world has never seen before.\\n\\nAt the center of this movement is a crucial conviction: that a nation exists to serve its citizens.\\n\\nAmericans want great schools for their children, safe neighborhoods for their families, and good jobs for themselves.\\n\\nThese are just and reasonable demands of righteous people and a righteous public.\\n\\nBut for too many of our citizens, a different reality exists: mothers and children trapped in poverty in our inner cities; rusted-out factories scattered like tombstones across the landscape of our nation; an education system, flush with cash, but which leaves our young and beautiful students deprived of all knowledge; and the crime and the gangs and the drugs that have stolen too many lives and robbed our country of so much unrealized potential.\\n\\nThis American carnage stops right here and stops right now.\\n\\nWe are one nation - and their pain is our pain. Their dreams are our dreams; and their success will be our success. We share one heart, one home, and one glorious destiny.\\n\\nThe oath of office I take today is an oath of allegiance to all Americans.\\n\\nFor many decades, we've enriched foreign industry at the expense of American industry; subsidized the armies of other countries while allowing for the very sad depletion of our military; we've defended other nations' borders while refusing to defend our own; and spent trillions and trillions of dollars overseas while America's infrastructure has fallen into disrepair and decay.\\n\\nWe've made other countries rich while the wealth, strength, and confidence of our country has dissipated over the horizon.\\n\\nOne by one, the factories shuttered and left our shores, with not even a thought about the millions and millions of American workers that were left behind.\\n\\nThe wealth of our middle class has been ripped from their homes and then redistributed all across the world.\\n\\nBut that is the past. And now we are looking only to the future.\\n\\nWe assembled here today are issuing a new decree to be heard in every city, in every foreign capital, and in every hall of power.\\n\\nFrom this day forward, a new vision will govern our land.\\n\\nFrom this day forward, it's going to be only America first, America first.\\n\\nEvery decision on trade, on taxes, on immigration, on foreign affairs, will be made to benefit American workers and American families.\\n\\nWe must protect our borders from the ravages of other countries making our products, stealing our companies, and destroying our jobs. Protection will lead to great prosperity and strength.\\n\\nI will fight for you with every breath in my body - and I will never, ever let you down.\\n\\nAmerica will start winning again, winning like never before.\\n\\nWe will bring back our jobs. We will bring back our borders. We will bring back our wealth. And we will bring back our dreams.\\n\\nWe will build new roads, and highways, and bridges, and airports, and tunnels, and railways all across our wonderful nation.\\n\\nWe will get our people off of welfare and back to work - rebuilding our country with American hands and American labor.\\n\\nWe will follow two simple rules: buy American and hire American.\\n\\nWe will seek friendship and goodwill with the nations of the world - but we do so with the understanding that it is the right of all nations to put their own interests first.\\n\\nWe do not seek to impose our way of life on anyone, but rather to let it shine as an example for everyone to follow.\\n\\nWe will reinforce old alliances and form new ones - and unite the civilized world against radical Islamic terrorism, which we will eradicate from the face of the Earth.\\n\\nAt the bedrock of our politics will be a total allegiance to the United States of America, and through our loyalty to our country, we will rediscover our loyalty to each other.\\n\\nWhen you open your heart to patriotism, there is no room for prejudice.\\n\\nThe Bible tells us: \\\"How good and pleasant it is when God's people live together in unity.\\\"\\n\\nWe must speak our minds openly, debate our disagreements honestly, but always pursue solidarity.\\n\\nWhen America is united, America is totally unstoppable.\\n\\nThere should be no fear - we are protected, and we will always be protected.\\n\\nWe will be protected by the great men and women of our military and law enforcement and, most importantly, we are protected by God.\\n\\nFinally, we must think big and dream even bigger.\\n\\nIn America, we understand that a nation is only living as long as it is striving.\\n\\nWe will no longer accept politicians who are all talk and no action - constantly complaining but never doing anything about it.\\n\\nThe time for empty talk is over.\\n\\nNow arrives the hour of action.\\n\\nDo not let anyone tell you it cannot be done. No challenge can match the heart and fight and spirit of America.\\n\\nWe will not fail. Our country will thrive and prosper again.\\n\\nWe stand at the birth of a new millennium, ready to unlock the mysteries of space, to free the Earth from the miseries of disease, and to harness the energies, industries and technologies of tomorrow.\\n\\nA new national pride will stir ourselves, lift our sights, and heal our divisions.\\n\\nIt is time to remember that old wisdom our soldiers will never forget: that whether we are black or brown or white, we all bleed the same red blood of patriots, we all enjoy the same glorious freedoms, and we all salute the same great American Flag.\\n\\nAnd whether a child is born in the urban sprawl of Detroit or the windswept plains of Nebraska, they look up at the same night sky, they fill their heart with the same dreams, and they are infused with the breath of life by the same almighty Creator.\\n\\nSo to all Americans, in every city near and far, small and large, from mountain to mountain, and from ocean to ocean, hear these words:\\n\\nYou will never be ignored again.\\n\\nYour voice, your hopes, and your dreams, will define our American destiny. And your courage and goodness and love will forever guide us along the way.\\n\\nTogether, we will make America strong again.\\n\\nWe will make America wealthy again.\\n\\nWe will make America proud again.\\n\\nWe will make America safe again.\\n\\nAnd, yes, together, we will make America great again. Thank you, God bless you, and God bless America.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjgHBZfOzueh",
    "outputId": "d7f16710-5721-4aa6-9d90-41fc076ba8f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberts 14 21 PERSON\n",
      "Carter 33 39 PERSON\n",
      "Clinton 51 58 PERSON\n",
      "Bush 70 74 PERSON\n",
      "Obama 86 91 PERSON\n",
      "Americans 100 109 NORP\n",
      "America 168 175 GPE\n",
      "America 332 339 GPE\n",
      "Every four years 469 485 DATE\n",
      "Obama 602 607 GPE\n",
      "First 612 617 ORDINAL\n",
      "Michelle Obama 623 637 PERSON\n",
      "Today 729 734 DATE\n",
      "today 790 795 DATE\n",
      "Washington DC 935 948 GPE\n",
      "Capital 1033 1040 ORG\n",
      "Washington 1117 1127 GPE\n",
      "today 1695 1700 DATE\n",
      "America 1734 1741 GPE\n",
      "the United States of America 1799 1827 GPE\n",
      "January 20, 2017 1967 1983 DATE\n",
      "the tens of millions 2187 2207 MONEY\n",
      "Americans 2400 2409 NORP\n",
      "American 3059 3067 NORP\n",
      "one 3122 3125 CARDINAL\n",
      "one 3251 3254 CARDINAL\n",
      "one 3265 3268 CARDINAL\n",
      "today 3314 3319 DATE\n",
      "Americans 3352 3361 NORP\n",
      "many decades 3368 3380 DATE\n",
      "American 3432 3440 NORP\n",
      "trillions and trillions 3633 3656 QUANTITY\n",
      "America 3683 3690 GPE\n",
      "decay 3738 3743 PERSON\n",
      "One 3870 3873 CARDINAL\n",
      "American 3986 3994 NORP\n",
      "today 4221 4226 DATE\n",
      "this day 4339 4347 DATE\n",
      "this day 4398 4406 DATE\n",
      "America 4438 4445 GPE\n",
      "first 4446 4451 ORDINAL\n",
      "America 4453 4460 GPE\n",
      "first 4461 4466 ORDINAL\n",
      "American 4564 4572 NORP\n",
      "American 4585 4593 NORP\n",
      "America 4885 4892 GPE\n",
      "American 5286 5294 NORP\n",
      "American 5305 5313 NORP\n",
      "two 5337 5340 CARDINAL\n",
      "American 5359 5367 NORP\n",
      "American 5377 5385 NORP\n",
      "first 5556 5561 ORDINAL\n",
      "Islamic 5780 5787 NORP\n",
      "Earth 5844 5849 LOC\n",
      "the United States of America 5913 5941 GPE\n",
      "Bible 6107 6112 WORK_OF_ART\n",
      "America 6299 6306 GPE\n",
      "America 6318 6325 GPE\n",
      "America 6616 6623 GPE\n",
      "the hour 6871 6879 TIME\n",
      "America 6995 7002 GPE\n",
      "a new millennium 7092 7108 DATE\n",
      "Earth 7162 7167 LOC\n",
      "American Flag 7588 7601 ORG\n",
      "Detroit 7655 7662 GPE\n",
      "Nebraska 7690 7698 GPE\n",
      "the same night 7716 7730 TIME\n",
      "Americans 7866 7875 NORP\n",
      "American 8084 8092 NORP\n",
      "America 8202 8209 GPE\n",
      "America 8238 8245 GPE\n",
      "America 8275 8282 GPE\n",
      "America 8310 8317 GPE\n",
      "America 8364 8371 GPE\n",
      "God 8396 8399 PERSON\n",
      "America 8425 8432 GPE\n"
     ]
    }
   ],
   "source": [
    "trump_ann_spacy = nlp_spacy(trump)\n",
    "for ent in trump_ann_spacy.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv_5ZsFo5vzo"
   },
   "source": [
    "### Morphological features\n",
    "\n",
    "A lemma can be *inflected* with **morphological features** to produce a \"surface form\". Examples of morpholological features include case, number, verb form, tense, person and mood. Spacy conducts morphological analysis as part of its pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KaicrD30HpG",
    "outputId": "f329aa17-37a7-4ab2-e182-a423fb8ddbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walked walk VERB VBD ['Tense=Past', 'VerbForm=Fin']\n",
      "walk walk VERB MD ['VerbForm=Inf']\n",
      "walking walk VERB VBG ['Aspect=Prog', 'Tense=Pres', 'VerbForm=Part']\n",
      "walking walk VERB VBG ['Aspect=Prog', 'Tense=Pres', 'VerbForm=Part']\n"
     ]
    }
   ],
   "source": [
    "ann2 = nlp_spacy(\"I walked the dog yesterday.\")\n",
    "print(ann2[1],ann2[1].lemma_,ann2[1].pos_,ann2[1].tag_, [mf for mf in ann2[1].morph])\n",
    "ann3 = nlp_spacy(\"I will walk the dog tomorrow.\")\n",
    "print(ann3[2],ann3[2].lemma_,ann3[2].pos_,ann3[1].tag_,[mf for mf in ann3[2].morph])\n",
    "ann4 = nlp_spacy(\"I am walking the dog.\")\n",
    "print(ann4[2],ann4[2].lemma_,ann4[2].pos_,ann4[2].tag_,[mf for mf in ann4[2].morph])\n",
    "ann5 = nlp_spacy(\"I was walking the dog.\")\n",
    "print(ann5[2],ann5[2].lemma_,ann5[2].pos_,ann5[2].tag_,[mf for mf in ann5[2].morph])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43s1VR0255cK"
   },
   "source": [
    "I invite the grammar afficionados among you to assess whether those were all as expected. The first two are tagged as finite present and past tense respectively. The third is labeled as a modal verb in infinitive form. The last two are labeled identically -- with detailed POS tag of VBG (\"gerund\") -- and progressive/ongoing present participle (despite the last being past).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOwqQA52u_Y_"
   },
   "source": [
    "### Noun phrases\n",
    " As part of its dependency parsing, Spacy will isolate **noun chunks** or **noun phrases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "anTGYjbe2vDE",
    "outputId": "8de2d772-dc4f-4624-8c6b-2dfcd6610446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "docnp = nlp_spacy(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in docnp.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5Ll7yi26A2s"
   },
   "source": [
    "### Dependency parse trees\n",
    "\n",
    "Dependency parsing is a complex subject we'll discuss in more detail separately. For present purposes, I'll just show some basics.\n",
    "\n",
    "Spacy's dependency parse is a tree that can be navigated like one. Every word has exactly one head, one word (or \"root\") that points to it via an arc. The example from the documentation looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "pi1QUHWj27yN",
    "outputId": "acd0ae22-f678-4aa2-ec1f-3cbd81ae6c6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"efd60d1de30c45ff83db66681ed75520-0\" class=\"displacy\" width=\"680\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Autonomous</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">cars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">shift</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">insurance</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">liability</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">toward</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">manufacturers</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-efd60d1de30c45ff83db66681ed75520-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-efd60d1de30c45ff83db66681ed75520-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-efd60d1de30c45ff83db66681ed75520-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-efd60d1de30c45ff83db66681ed75520-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-efd60d1de30c45ff83db66681ed75520-0-2\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-efd60d1de30c45ff83db66681ed75520-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-efd60d1de30c45ff83db66681ed75520-0-3\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-efd60d1de30c45ff83db66681ed75520-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M405.0,139.0 L413.0,127.0 397.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-efd60d1de30c45ff83db66681ed75520-0-4\" stroke-width=\"2px\" d=\"M250,137.0 C250,2.0 500.0,2.0 500.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-efd60d1de30c45ff83db66681ed75520-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M500.0,139.0 L508.0,127.0 492.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-efd60d1de30c45ff83db66681ed75520-0-5\" stroke-width=\"2px\" d=\"M520,137.0 C520,92.0 580.0,92.0 580.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-efd60d1de30c45ff83db66681ed75520-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M580.0,139.0 L588.0,127.0 572.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docnp = nlp_spacy(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "displacy.render(docnp, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQtPHAUb6Kbs"
   },
   "source": [
    "So we iterate over words to find an arc of interest \"from below.\" Specifically, in this example, we search for a verb that has a subject (a verb with an \"nsubj\" arc leading from it) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ezm-E8WP3DUG",
    "outputId": "a9ae5733-e19f-4252-9b91-5ab39d6e2283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{shift}\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import nsubj, VERB\n",
    "# Finding a verb with a subject from below — good\n",
    "verbs = set()\n",
    "for possible_subject in docnp:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaoKneeB6QtV"
   },
   "source": [
    "The verb \"shift\" is the only verb with a subject (\"cars\"). (The core subject-verb-object construction is \"cars shift liability.\")\n",
    "\n",
    "Spacy provides attributes that can be used to traverse the tree. For example attribute `lefts` contains the children nodes to the left of a given node, and `rights` contains the children nodes to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXSayLmY3S4n",
    "outputId": "b575acd6-7a13-4969-c824-c45c7aca0b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left cars nsubj\n",
      "right liability dobj\n",
      "right toward prep\n"
     ]
    }
   ],
   "source": [
    "for left in docnp[2].lefts:\n",
    "  print(\"left\", left,left.dep_)\n",
    "for right in docnp[2].rights:\n",
    "  print(\"right\",right,right.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynSNQ-iy6ZQu"
   },
   "source": [
    "So, \"shift\" has three children, the subject \"cars\" to its left, the direct object \"liability\" to its right, and the preposition \"toward\" to its right.\n",
    "\n",
    "Dependency parsing is the basic foundation for many information extraction applications, such as political event data production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3qUUIRT6dL9"
   },
   "source": [
    "## Stanza (formerly Stanford NLP) and coreNLP\n",
    "\n",
    "\n",
    "Stanza -- formerly StanfordNLP -- is a Python library from the Stanford NLP group. Stanza provides a wrapper to coreNLP, the research group's Java library, and it inherits coreNLP functionality.  \n",
    "\n",
    "The official description:\n",
    "\n",
    "> Stanza is a Python natural language analysis package. It contains tools, which can be used in a pipeline, to convert a string containing human language text into lists of sentences and words, to generate base forms of those words, their parts of speech and morphological features, to give a syntactic structure dependency parse, and to recognize named entities. The toolkit is designed to be parallel among more than 70 languages, using the Universal Dependencies formalism.\n",
    "\n",
    "> Stanza is built with highly accurate neural network components that also enable efficient training and evaluation with your own annotated data. The modules are built on top of the PyTorch library. You will get much faster performance if you run the software on a GPU-enabled machine.\n",
    "\n",
    "> In addition, Stanza includes a Python interface to the CoreNLP Java package and inherits additional functionality from there, such as constituency parsing, coreference resolution, and linguistic pattern matching.\n",
    "\n",
    "> To summarize, Stanza features:\n",
    "\n",
    "> Native Python implementation requiring minimal efforts to set up;\n",
    "\n",
    "> Full neural network pipeline for robust text analytics, including tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging, dependency parsing, and named entity recognition;\n",
    "\n",
    "> Pretrained neural models supporting 66 (human) languages;\n",
    "\n",
    "> A stable, officially maintained Python interface to CoreNLP.\n",
    "\n",
    "Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020.\n",
    "\n",
    "Stanford **coreNLP** has historically been one of the standard, most commonly used, NLP engines. From its official page (https://stanfordnlp.github.io/CoreNLP/):\n",
    "\n",
    "> CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations. CoreNLP currently supports 6 languages: Arabic, Chinese, English, French, German, and Spanish.\n",
    "\n",
    "Stanza is now the way to access coreNLP through Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FX0k3GJFjJ33",
    "outputId": "0c0c06b6-4597-479d-9209-c8cb2a3921f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.6.1-py3-none-any.whl (881 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting emoji (from stanza)\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.8.0 stanza-1.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDe0cZbuwW3q"
   },
   "source": [
    "### The standard Stanza pipeline (tokens, pos, lemmas, dependency parse, sentiment, ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579,
     "referenced_widgets": [
      "678647a588df4f778a1d0e7981ff7792",
      "3fec37b560d043878ef0ffdfac4d84a1",
      "93ac252d21e44837a6ae7ebed50d58bc",
      "40f3704a3eba4026b8ebf6858848df7e",
      "d38bc9778a2c4c04af362c57bfdebf3d",
      "3d51779827ee4eba97ac6aa2e7a71b0f",
      "d8d41f8bed744ad59526baff61716604",
      "64e5f4a97e4a48029bbc06745bd0bcde",
      "ae88519205ee4aaab48b4080d2dc8a04",
      "d954d70670c546caa2d5c818b12098df",
      "e027d71b5ef5492f8e93b732b22f9e8e",
      "6ef9dea7cabc4da7a4b445237e359032",
      "900df244a85445b2b7e3e6648764a9f9",
      "f83e103244c74f3f8898ae8311037cf4",
      "6e2c5a3a18ec4f21b3906afa6fc2d546",
      "2616895187194a5f8eb59fe28d6f91dc",
      "3e906d99bdde40979763c10366f2d202",
      "0565ff4b466843d6814a60aefa3801b8",
      "af6c399158d3473b9a310eda81555bd5",
      "b67e49bf262f42909a01662d2a3b5604",
      "0ee4fc62e516495c84408796e8c8ca53",
      "624de64a9eb94210bd49e91a71e880eb",
      "d95b11f49bef4d12b2eb80a47a940c5c",
      "6cd4b27ff27244a992233ae17608ad7e",
      "8bd11edc76ec4acabc2636bd820f9a31",
      "6d4d99fadc1e404baaad5b6c94cb91b8",
      "102fca374e4d49d0aafa74ccc8972abd",
      "c2888beaf30f4b1e80d3123924dd25d9",
      "30a0ebb030d441cdb76e73bc2fdf2d33",
      "0d59c5c215b647a6aa75e7dbd2f1a50b",
      "fcc370893d1444a8b16a57a154f54b6d",
      "a6562d25fdfa46209bb29c43a075b37d",
      "acc5e1ca2da8429fbe1ea30100960b25"
     ]
    },
    "id": "DPAwNpYgjpj3",
    "outputId": "3d105b94-1217-418f-a7e5-8363143bd550"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678647a588df4f778a1d0e7981ff7792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef9dea7cabc4da7a4b445237e359032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Finished downloading models and saved to /root/stanza_resources.\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95b11f49bef4d12b2eb80a47a940c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "INFO:stanza:Using device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: constituency\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Loading: sentiment\n",
      "INFO:stanza:Loading: ner\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbv4rfSvmKD3"
   },
   "source": [
    "Note that we downloaded the \"default\" processors for the NLP pipeline. You can specify *which* specific processors to use for any given task, if options exist in that language as well as substitute your own.\n",
    "\n",
    "The pipeline includes (as much as) a tokenizer, a POS tagger, a lemmatizer, a dependency parser, a sentiment analyzer, and a named-entity recognizer.\n",
    "\n",
    "We apply the pipeline to our text and assign it to a Document object, which will include the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SiB0-JDkiDX"
   },
   "outputs": [],
   "source": [
    "annotated_doc_stanza = nlp_stanza(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASfhiFpmnS0d"
   },
   "source": [
    "The Document now has attributes including `sentences`, a list of Sentence objects. Sentence objects have attributes that include a list of `tokens`, `words`, entities (`ents`), `dependencies`, and `sentiment` (if there was a sentiment processor).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcPvZXFlkvPF",
    "outputId": "88f063dc-4534-4cde-d759-3c48aa6fedbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN\n",
      "is be AUX\n",
      "looking look VERB\n",
      "at at SCONJ\n",
      "buying buy VERB\n",
      "U.K. U.K. PROPN\n",
      "startup startup NOUN\n",
      "for for ADP\n",
      "$ $ SYM\n",
      "1 1 NUM\n",
      "billion billion NUM\n"
     ]
    }
   ],
   "source": [
    "for sentence in annotated_doc_stanza.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma, word.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZw6_60-xRA5"
   },
   "source": [
    "### Named entities and dependency parse trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAtdLFr3z4kI"
   },
   "source": [
    "Entities and dependencies are provided in lists of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtZFHQcNkzqj",
    "outputId": "e3a8a152-559b-44be-916f-3dba75480c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Apple\",\n",
      "  \"type\": \"ORG\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 5\n",
      "}, {\n",
      "  \"text\": \"U.K.\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 27,\n",
      "  \"end_char\": 31\n",
      "}, {\n",
      "  \"text\": \"$1 billion\",\n",
      "  \"type\": \"MONEY\",\n",
      "  \"start_char\": 44,\n",
      "  \"end_char\": 54\n",
      "}]\n",
      "[({\n",
      "  \"id\": 3,\n",
      "  \"text\": \"looking\",\n",
      "  \"lemma\": \"look\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"Tense=Pres|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 9,\n",
      "  \"end_char\": 16\n",
      "}, 'nsubj', {\n",
      "  \"id\": 1,\n",
      "  \"text\": \"Apple\",\n",
      "  \"lemma\": \"Apple\",\n",
      "  \"upos\": \"PROPN\",\n",
      "  \"xpos\": \"NNP\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 3,\n",
      "  \"deprel\": \"nsubj\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 5\n",
      "}), ({\n",
      "  \"id\": 3,\n",
      "  \"text\": \"looking\",\n",
      "  \"lemma\": \"look\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"Tense=Pres|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 9,\n",
      "  \"end_char\": 16\n",
      "}, 'aux', {\n",
      "  \"id\": 2,\n",
      "  \"text\": \"is\",\n",
      "  \"lemma\": \"be\",\n",
      "  \"upos\": \"AUX\",\n",
      "  \"xpos\": \"VBZ\",\n",
      "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "  \"head\": 3,\n",
      "  \"deprel\": \"aux\",\n",
      "  \"start_char\": 6,\n",
      "  \"end_char\": 8\n",
      "}), ({\n",
      "  \"id\": 0,\n",
      "  \"text\": \"ROOT\"\n",
      "}, 'root', {\n",
      "  \"id\": 3,\n",
      "  \"text\": \"looking\",\n",
      "  \"lemma\": \"look\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"Tense=Pres|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 9,\n",
      "  \"end_char\": 16\n",
      "}), ({\n",
      "  \"id\": 5,\n",
      "  \"text\": \"buying\",\n",
      "  \"lemma\": \"buy\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"VerbForm=Ger\",\n",
      "  \"head\": 3,\n",
      "  \"deprel\": \"advcl\",\n",
      "  \"start_char\": 20,\n",
      "  \"end_char\": 26\n",
      "}, 'mark', {\n",
      "  \"id\": 4,\n",
      "  \"text\": \"at\",\n",
      "  \"lemma\": \"at\",\n",
      "  \"upos\": \"SCONJ\",\n",
      "  \"xpos\": \"IN\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"mark\",\n",
      "  \"start_char\": 17,\n",
      "  \"end_char\": 19\n",
      "}), ({\n",
      "  \"id\": 3,\n",
      "  \"text\": \"looking\",\n",
      "  \"lemma\": \"look\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"Tense=Pres|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 9,\n",
      "  \"end_char\": 16\n",
      "}, 'advcl', {\n",
      "  \"id\": 5,\n",
      "  \"text\": \"buying\",\n",
      "  \"lemma\": \"buy\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"VerbForm=Ger\",\n",
      "  \"head\": 3,\n",
      "  \"deprel\": \"advcl\",\n",
      "  \"start_char\": 20,\n",
      "  \"end_char\": 26\n",
      "}), ({\n",
      "  \"id\": 7,\n",
      "  \"text\": \"startup\",\n",
      "  \"lemma\": \"startup\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NN\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"start_char\": 32,\n",
      "  \"end_char\": 39\n",
      "}, 'compound', {\n",
      "  \"id\": 6,\n",
      "  \"text\": \"U.K.\",\n",
      "  \"lemma\": \"U.K.\",\n",
      "  \"upos\": \"PROPN\",\n",
      "  \"xpos\": \"NNP\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 7,\n",
      "  \"deprel\": \"compound\",\n",
      "  \"start_char\": 27,\n",
      "  \"end_char\": 31\n",
      "}), ({\n",
      "  \"id\": 5,\n",
      "  \"text\": \"buying\",\n",
      "  \"lemma\": \"buy\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"VerbForm=Ger\",\n",
      "  \"head\": 3,\n",
      "  \"deprel\": \"advcl\",\n",
      "  \"start_char\": 20,\n",
      "  \"end_char\": 26\n",
      "}, 'obj', {\n",
      "  \"id\": 7,\n",
      "  \"text\": \"startup\",\n",
      "  \"lemma\": \"startup\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NN\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"start_char\": 32,\n",
      "  \"end_char\": 39\n",
      "}), ({\n",
      "  \"id\": 9,\n",
      "  \"text\": \"$\",\n",
      "  \"lemma\": \"$\",\n",
      "  \"upos\": \"SYM\",\n",
      "  \"xpos\": \"$\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"obl\",\n",
      "  \"start_char\": 44,\n",
      "  \"end_char\": 45\n",
      "}, 'case', {\n",
      "  \"id\": 8,\n",
      "  \"text\": \"for\",\n",
      "  \"lemma\": \"for\",\n",
      "  \"upos\": \"ADP\",\n",
      "  \"xpos\": \"IN\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"case\",\n",
      "  \"start_char\": 40,\n",
      "  \"end_char\": 43\n",
      "}), ({\n",
      "  \"id\": 5,\n",
      "  \"text\": \"buying\",\n",
      "  \"lemma\": \"buy\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VBG\",\n",
      "  \"feats\": \"VerbForm=Ger\",\n",
      "  \"head\": 3,\n",
      "  \"deprel\": \"advcl\",\n",
      "  \"start_char\": 20,\n",
      "  \"end_char\": 26\n",
      "}, 'obl', {\n",
      "  \"id\": 9,\n",
      "  \"text\": \"$\",\n",
      "  \"lemma\": \"$\",\n",
      "  \"upos\": \"SYM\",\n",
      "  \"xpos\": \"$\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"obl\",\n",
      "  \"start_char\": 44,\n",
      "  \"end_char\": 45\n",
      "}), ({\n",
      "  \"id\": 11,\n",
      "  \"text\": \"billion\",\n",
      "  \"lemma\": \"billion\",\n",
      "  \"upos\": \"NUM\",\n",
      "  \"xpos\": \"CD\",\n",
      "  \"feats\": \"NumForm=Word|NumType=Card\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"nummod\",\n",
      "  \"start_char\": 47,\n",
      "  \"end_char\": 54\n",
      "}, 'compound', {\n",
      "  \"id\": 10,\n",
      "  \"text\": \"1\",\n",
      "  \"lemma\": \"1\",\n",
      "  \"upos\": \"NUM\",\n",
      "  \"xpos\": \"CD\",\n",
      "  \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "  \"head\": 11,\n",
      "  \"deprel\": \"compound\",\n",
      "  \"start_char\": 45,\n",
      "  \"end_char\": 46\n",
      "}), ({\n",
      "  \"id\": 9,\n",
      "  \"text\": \"$\",\n",
      "  \"lemma\": \"$\",\n",
      "  \"upos\": \"SYM\",\n",
      "  \"xpos\": \"$\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"obl\",\n",
      "  \"start_char\": 44,\n",
      "  \"end_char\": 45\n",
      "}, 'nummod', {\n",
      "  \"id\": 11,\n",
      "  \"text\": \"billion\",\n",
      "  \"lemma\": \"billion\",\n",
      "  \"upos\": \"NUM\",\n",
      "  \"xpos\": \"CD\",\n",
      "  \"feats\": \"NumForm=Word|NumType=Card\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"nummod\",\n",
      "  \"start_char\": 47,\n",
      "  \"end_char\": 54\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "for sentence in annotated_doc_stanza.sentences:\n",
    "    print(sentence.ents)\n",
    "    print(sentence.dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lduZEt7q747d"
   },
   "source": [
    "Let's look again at the entities extracted from the Trump inaugural to compare with those from spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLzctuuQ779i"
   },
   "outputs": [],
   "source": [
    "trump_ann_stanza = nlp_stanza(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDNSpNyv8OWn",
    "outputId": "44ed3926-8c95-43dc-9806-1ab03648db35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Roberts\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 14,\n",
      "  \"end_char\": 21\n",
      "}, {\n",
      "  \"text\": \"Carter\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 33,\n",
      "  \"end_char\": 39\n",
      "}, {\n",
      "  \"text\": \"Clinton\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 51,\n",
      "  \"end_char\": 58\n",
      "}, {\n",
      "  \"text\": \"Bush\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 70,\n",
      "  \"end_char\": 74\n",
      "}, {\n",
      "  \"text\": \"Obama\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 86,\n",
      "  \"end_char\": 91\n",
      "}, {\n",
      "  \"text\": \"Americans\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 100,\n",
      "  \"end_char\": 109\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 168,\n",
      "  \"end_char\": 175\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 332,\n",
      "  \"end_char\": 339\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"Every four years\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 469,\n",
      "  \"end_char\": 485\n",
      "}, {\n",
      "  \"text\": \"Obama\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 602,\n",
      "  \"end_char\": 607\n",
      "}, {\n",
      "  \"text\": \"First\",\n",
      "  \"type\": \"ORDINAL\",\n",
      "  \"start_char\": 612,\n",
      "  \"end_char\": 617\n",
      "}, {\n",
      "  \"text\": \"Michelle Obama\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 623,\n",
      "  \"end_char\": 637\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"Today\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 729,\n",
      "  \"end_char\": 734\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"today\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 790,\n",
      "  \"end_char\": 795\n",
      "}, {\n",
      "  \"text\": \"Administration\",\n",
      "  \"type\": \"ORG\",\n",
      "  \"start_char\": 842,\n",
      "  \"end_char\": 856\n",
      "}, {\n",
      "  \"text\": \"one\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 877,\n",
      "  \"end_char\": 880\n",
      "}, {\n",
      "  \"text\": \"Washington DC\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 935,\n",
      "  \"end_char\": 948\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"Capital\",\n",
      "  \"type\": \"ORG\",\n",
      "  \"start_char\": 1033,\n",
      "  \"end_char\": 1040\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"Washington\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 1117,\n",
      "  \"end_char\": 1127\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"today\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 1695,\n",
      "  \"end_char\": 1700\n",
      "}, {\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 1734,\n",
      "  \"end_char\": 1741\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"the United States of America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 1799,\n",
      "  \"end_char\": 1827\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"January 20, 2017\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 1967,\n",
      "  \"end_char\": 1983\n",
      "}, {\n",
      "  \"text\": \"the day\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 2007,\n",
      "  \"end_char\": 2014\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"tens of millions\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 2191,\n",
      "  \"end_char\": 2207\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"Americans\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 2400,\n",
      "  \"end_char\": 2409\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 3059,\n",
      "  \"end_char\": 3067\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"one\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 3122,\n",
      "  \"end_char\": 3125\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"one\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 3240,\n",
      "  \"end_char\": 3243\n",
      "}, {\n",
      "  \"text\": \"one\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 3251,\n",
      "  \"end_char\": 3254\n",
      "}, {\n",
      "  \"text\": \"one\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 3265,\n",
      "  \"end_char\": 3268\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"today\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 3314,\n",
      "  \"end_char\": 3319\n",
      "}, {\n",
      "  \"text\": \"Americans\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 3352,\n",
      "  \"end_char\": 3361\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"many decades\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 3368,\n",
      "  \"end_char\": 3380\n",
      "}, {\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 3432,\n",
      "  \"end_char\": 3440\n",
      "}, {\n",
      "  \"text\": \"trillions and trillions of dollars\",\n",
      "  \"type\": \"MONEY\",\n",
      "  \"start_char\": 3633,\n",
      "  \"end_char\": 3667\n",
      "}, {\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 3683,\n",
      "  \"end_char\": 3690\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"One\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 3870,\n",
      "  \"end_char\": 3873\n",
      "}, {\n",
      "  \"text\": \"one\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 3877,\n",
      "  \"end_char\": 3880\n",
      "}, {\n",
      "  \"text\": \"millions and millions\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 3961,\n",
      "  \"end_char\": 3982\n",
      "}, {\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 3986,\n",
      "  \"end_char\": 3994\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"today\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 4221,\n",
      "  \"end_char\": 4226\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"this day\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 4339,\n",
      "  \"end_char\": 4347\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 4438,\n",
      "  \"end_char\": 4445\n",
      "}, {\n",
      "  \"text\": \"first\",\n",
      "  \"type\": \"ORDINAL\",\n",
      "  \"start_char\": 4446,\n",
      "  \"end_char\": 4451\n",
      "}, {\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 4453,\n",
      "  \"end_char\": 4460\n",
      "}, {\n",
      "  \"text\": \"first\",\n",
      "  \"type\": \"ORDINAL\",\n",
      "  \"start_char\": 4461,\n",
      "  \"end_char\": 4466\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 4564,\n",
      "  \"end_char\": 4572\n",
      "}, {\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 4585,\n",
      "  \"end_char\": 4593\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 4885,\n",
      "  \"end_char\": 4892\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 5286,\n",
      "  \"end_char\": 5294\n",
      "}, {\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 5305,\n",
      "  \"end_char\": 5313\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"two\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 5337,\n",
      "  \"end_char\": 5340\n",
      "}, {\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 5359,\n",
      "  \"end_char\": 5367\n",
      "}, {\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 5377,\n",
      "  \"end_char\": 5385\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"first\",\n",
      "  \"type\": \"ORDINAL\",\n",
      "  \"start_char\": 5556,\n",
      "  \"end_char\": 5561\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"Islamic\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 5780,\n",
      "  \"end_char\": 5787\n",
      "}, {\n",
      "  \"text\": \"Earth\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 5844,\n",
      "  \"end_char\": 5849\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"the United States of America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 5913,\n",
      "  \"end_char\": 5941\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"Bible\",\n",
      "  \"type\": \"WORK_OF_ART\",\n",
      "  \"start_char\": 6107,\n",
      "  \"end_char\": 6112\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 6299,\n",
      "  \"end_char\": 6306\n",
      "}, {\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 6318,\n",
      "  \"end_char\": 6325\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 6616,\n",
      "  \"end_char\": 6623\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"the hour\",\n",
      "  \"type\": \"TIME\",\n",
      "  \"start_char\": 6871,\n",
      "  \"end_char\": 6879\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 6995,\n",
      "  \"end_char\": 7002\n",
      "}]\n",
      "[]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"the birth of a new millennium\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 7079,\n",
      "  \"end_char\": 7108\n",
      "}, {\n",
      "  \"text\": \"Earth\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 7162,\n",
      "  \"end_char\": 7167\n",
      "}, {\n",
      "  \"text\": \"tomorrow\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 7258,\n",
      "  \"end_char\": 7266\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"American Flag\",\n",
      "  \"type\": \"ORG\",\n",
      "  \"start_char\": 7588,\n",
      "  \"end_char\": 7601\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"Detroit\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 7655,\n",
      "  \"end_char\": 7662\n",
      "}, {\n",
      "  \"text\": \"Nebraska\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 7690,\n",
      "  \"end_char\": 7698\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"Americans\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 7866,\n",
      "  \"end_char\": 7875\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"American\",\n",
      "  \"type\": \"NORP\",\n",
      "  \"start_char\": 8084,\n",
      "  \"end_char\": 8092\n",
      "}]\n",
      "[]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 8202,\n",
      "  \"end_char\": 8209\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 8238,\n",
      "  \"end_char\": 8245\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 8275,\n",
      "  \"end_char\": 8282\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 8310,\n",
      "  \"end_char\": 8317\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 8364,\n",
      "  \"end_char\": 8371\n",
      "}]\n",
      "[{\n",
      "  \"text\": \"America\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 8425,\n",
      "  \"end_char\": 8432\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "for sentence in trump_ann_stanza.sentences:\n",
    "    print(sentence.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNEGBBft-B8z"
   },
   "source": [
    "## NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK is the longest established NLP library. It has lots of tools for lots of NLP tasks in lots of languages, including classification, tokenization, stemming, tagging, parsing, semantic reasoning. It interfaces to “over 50 corpora and lexical resources such as WordNet\" (many standard corpora are available directly from NLTK). It is easier to tweak / modify / extend functionality in NLTK than spaCy and there is a large user community, so it is easy to find lots of examples, etc.  There is a free book that serves as most people's entree into NLTK: Steven Bird, Ewan Klein, and Edward Loper. “Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit” updated for Python 3 and NLTK3: http://www.nltk.org/book.\n",
    "\n",
    "NLTK is still widely used, but it is not integrated with neural network / word embedding approaches and definitely not as hip anymore.\n",
    "\n",
    "There isn't a generic \"pipeline\" command that does a default series of sequence labeling tasks, as there is with spaCy and Stanza. You need to download and apply models/resources for different tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJ-SG5Kzp1TE",
    "outputId": "78f05b37-d40f-42af-ffcc-b620f2a96a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kKipwOX0cA2"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "There are roughly 20 tokenizers available in NLTK. The  generic sounding `word_tokenize` and `sent_tokenize` commands load NLTK's default recommended `punkt` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjjMWnPPrB7L",
    "outputId": "8ebf3043-4413-4432-e51a-41a09e995401"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk import word_tokenize, sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BcLirmL4rJHb",
    "outputId": "e00988fb-e3b7-4d49-cd26-69f31c7ab8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "tok_nltk = word_tokenize(sent)\n",
    "print(tok_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI6zL9Rq0x9x"
   },
   "source": [
    "### POS tagging\n",
    "\n",
    "There are about a dozen different taggers available in the `nltk.tag` module. The one that seems to be used in most examples is `pos_tag`. It is applied to tokenized text, so we begin to see a pipeline forming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PL9aEofPsWIK",
    "outputId": "e6f66fce-c8fc-4279-f904-72febb88e404"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'), ('U.K.', 'NNP'), ('startup', 'NN'), ('for', 'IN'), ('$', '$'), ('1', 'CD'), ('billion', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = pos_tag(tok_nltk)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY71SZl24Z8j"
   },
   "source": [
    "### Named entities\n",
    "\n",
    "In turn, the tagged object can be passed to a named entity \"chunker.\" A chunker divides the tokens into \"chunks\" -- non-overlapping sequences of tokens. This is also known as shallow parsing. The recommended NLTK named entity chunker is accessed through the `ne_chunk` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BmNMmIyKaJO",
    "outputId": "17c09d21-4051-43d8-ca4e-42ef3d8fb976"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Apple/NNP)\n",
      "  is/VBZ\n",
      "  looking/VBG\n",
      "  at/IN\n",
      "  buying/VBG\n",
      "  U.K./NNP\n",
      "  startup/NN\n",
      "  for/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "named_ents = nltk.ne_chunk(tagged, binary=False)\n",
    "print(named_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuQjVAFuZjXc"
   },
   "source": [
    "It identifies \"Apple\" as a \"geo-political entity,\" which probably isn't what you want (although it's not entirely wrong either, I guess). The `binary=False` option asks for these classifications into types of named entities -- person, GPE, etc. The default of `binary=True` just returns an indication that something is a named entity.\n",
    "\n",
    "Note that this returns an nltk Tree object, which needs to be traversed in a tree-like way for some purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TGqLERtX0vg"
   },
   "source": [
    "Let's look at our Trump example again and apply these pieces in a pipeline. We will first apply the sentence tokenizer, then the word tokenizer, then the POS tagger, then the named entity chunker. The Tree object output by the chunker has tokens *not* in named entities as leaves of the tree as well. These don't have the named entity label, though, so we'll just barrel through the Tree object brute force, look at every leaf, check for that label and output only those that have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLfVYDFSXKHQ",
    "outputId": "d63ae7fd-b698-4208-ea04-9a7a81e37e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORGANIZATION Justice Roberts\n",
      "PERSON Carter\n",
      "PERSON Clinton\n",
      "PERSON Bush\n",
      "PERSON Obama\n",
      "GPE America\n",
      "GPE America\n",
      "PERSON Obama\n",
      "PERSON First Lady Michelle Obama\n",
      "GPE Washington\n",
      "ORGANIZATION Capital\n",
      "GPE Washington\n",
      "GPE America\n",
      "GPE United States\n",
      "GPE America\n",
      "GPE Everyone\n",
      "GPE Americans\n",
      "GPE American\n",
      "GPE American\n",
      "GPE America\n",
      "GPE American\n",
      "GPE America\n",
      "GPE America\n",
      "GPE American\n",
      "GPE American\n",
      "GPE America\n",
      "GPE American\n",
      "GPE American\n",
      "GPE American\n",
      "GPE American\n",
      "ORGANIZATION Islamic\n",
      "LOCATION Earth\n",
      "GPE United States\n",
      "GPE America\n",
      "ORGANIZATION Bible\n",
      "PERSON God\n",
      "GPE America\n",
      "GPE America\n",
      "GPE America\n",
      "GPE America\n",
      "ORGANIZATION American Flag\n",
      "GPE Detroit\n",
      "GPE Nebraska\n",
      "GPE American\n",
      "GPE America\n",
      "GPE America\n",
      "GPE America\n",
      "GPE America\n",
      "GPE America\n",
      "PERSON God\n",
      "GPE America\n"
     ]
    }
   ],
   "source": [
    "for sent in nltk.sent_tokenize(trump):\n",
    "   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "      if hasattr(chunk, 'label'):\n",
    "         print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOe0c-pLbRYK"
   },
   "source": [
    "Not very subtle, compared to spaCy and Stanza. It's the only one that doesn't identify Justice Roberts as a person, identifies the Bible as an organization, and so on. I'd have to look more closely, but it appears that it *only* identifies capitalized noun phrases as named entities, which may or may not be desired behavior in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHXJGAb_6w5h"
   },
   "source": [
    "### Noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_TpyYhff3qF"
   },
   "source": [
    "Speaking of noun phrases ... noun phrase chunking in nltk requires you to define a pattern of parts of speech that you consider to be a noun phrase and then parse using regular expressions. I found dozens of different patters strewn about the web. A couple of them are demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUy_SFKadp8v",
    "outputId": "db0f56c0-3e27-4dcd-ce6e-1e1b88a569a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Autonomous/JJ\n",
      "  (NP cars/NNS shift/VB insurance/NN liability/NN)\n",
      "  toward/IN\n",
      "  manufacturers/NNS)\n",
      "(S\n",
      "  Autonomous/JJ\n",
      "  cars/NNS\n",
      "  shift/VB\n",
      "  (NP insurance/NN liability/NN)\n",
      "  toward/IN\n",
      "  manufacturers/NNS)\n"
     ]
    }
   ],
   "source": [
    "sent = \"Autonomous cars shift insurance liability toward manufacturers\"\n",
    "tagged_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "\n",
    "NPpattern1 = r\"\"\"NP: {(<V\\w+>|<NN\\w?>)+.*<NN\\w?>}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(NPpattern1)\n",
    "chunked_sent = chunkParser.parse(tagged_sent)\n",
    "print(chunked_sent)\n",
    "\n",
    "NPpattern2 = r\"\"\"\n",
    "    NP: {<JJ>*<NN>+}\n",
    "    {<JJ>*<NN><CC>*<NN>+}\n",
    "    \"\"\"\n",
    "chunkParser = nltk.RegexpParser(NPpattern2)\n",
    "chunked_sent = chunkParser.parse(tagged_sent)\n",
    "print(chunked_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cLo76bGguRK"
   },
   "source": [
    "The first one identifies \"cars shift insurance liability\" which is ... wrong. The second one identifies \"insurance liability\" which is right. Spacy also identified \"autonomous cars\" and the one-token \"manufacturers\" as noun phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQptXDyh7Hxj"
   },
   "source": [
    "### Stemming and Lemmatizing\n",
    "\n",
    "The `nltk.stem` module provides access to a few stemmers and one lemmatizer based on WordNet. The lemmatizer is worth a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdgHDw20NazL",
    "outputId": "510baf6b-d47c-4c65-d657-544a93c25ee1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for tok in tok_nltk:\n",
    "  print(lemmatizer.lemmatize(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32xWHewuPHXx"
   },
   "source": [
    "That probably isn't what you expected. This lemmatizer takes a part of speech argument, `pos` which defaults to \"noun\". So \"looking\" is assumed to be a noun, lemmatized to \"looking.\" It needs to know its a verb to lemmatize to \"look.\" What's more \"looks\" lemmatizes to \"look\" whether it's assumed to be \"he looks scared\" or \"he gave her some mean looks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6xrQQCHOkmz",
    "outputId": "c6f8ef9c-82c4-4fcc-e937-9f04021b9248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking\n",
      "look\n",
      "look\n",
      "look\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('looking'))\n",
    "print(lemmatizer.lemmatize('looks'))\n",
    "print(lemmatizer.lemmatize('looking', pos='v'))\n",
    "print(lemmatizer.lemmatize('looks', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjCyq7vsStxF"
   },
   "source": [
    "You can use the output of a part-of-speech tagger to improve this, but given that the POS tags don't map exactly onto the lemmatizer's options for the `pos` argument, this is a little bit convoluted. You can see an example (using the Stanford_POS_tagger) here: https://towardsdatascience.com/building-a-text-normalizer-using-nltk-ft-pos-tagger-e713e611db8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXCKRe7S7_pO"
   },
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IJ7LpEWUT8E"
   },
   "source": [
    "Dependency parsing is also a little convoluted in NLTK. From what I can tell, you can call different dependency parsers, but the dominant way to do this in NLTK calls the Stanford CoreNLP dependency parser. But that -- along with most of the parsers available from what I can tell -- gets Java involved, so I'll just point you to the documentation: http://www.nltk.org/api/nltk.parse.html#module-nltk.parse.corenlp. (If you just want to use coreNLP, I recommend you just use it through Stanza.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQHSp8xVpySt"
   },
   "source": [
    "## Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lpwej8NM_HOl",
    "outputId": "2df9d88f-4800-4585-9822-0f249d515af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.13.0-py3-none-any.whl (387 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.2/387.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3>=1.20.27 (from flair)\n",
      "  Downloading boto3-1.29.6-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bpemb>=0.3.2 (from flair)\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting conllu>=4.0 (from flair)\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair)\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.6.6)\n",
      "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.19.4)\n",
      "Collecting janome>=0.4.2 (from flair)\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langdetect>=1.0.9 (from flair)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.7.1)\n",
      "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.1.0)\n",
      "Collecting mpld3>=0.3 (from flair)\n",
      "  Downloading mpld3-0.5.9-py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.2/201.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pptree>=3.1 (from flair)\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2023.6.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.2)\n",
      "Collecting segtok>=1.5.11 (from flair)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.1)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
      "  Downloading transformer_smaller_training_vocab-0.3.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.35.2)\n",
      "Collecting urllib3<2.0.0,>=1.0.0 (from flair)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair)\n",
      "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair)\n",
      "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Collecting botocore<1.33.0,>=1.32.6 (from boto3>=1.20.27->flair)\n",
      "  Downloading botocore-1.32.6-py3-none-any.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3>=1.20.27->flair)\n",
      "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
      "Collecting sentencepiece (from bpemb>=0.3.2->flair)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy>=6.1.0->flair)\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.13.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.11.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (6.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2023.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (23.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.44.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.1.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.2.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.2.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.1.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.4.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (3.20.3)\n",
      "Collecting accelerate>=0.20.3 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.5)\n",
      "Building wheels for collected packages: langdetect, pptree, sqlitedict\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=9b7335c97cb671f1bc437cb62d767715fd33115164d2b2d0bd6a775b9429cd1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4607 sha256=9ad31c1bc392d382074bb90b759762fb71c5f5203dcbb5e7007ba525bc85e9ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=5f1aaff99d580bdc7b67fbcd8fd961ae1cea921796f00775c06d355a3f745d23\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "Successfully built langdetect pptree sqlitedict\n",
      "Installing collected packages: wcwidth, sqlitedict, sentencepiece, pptree, janome, urllib3, semver, segtok, langdetect, jmespath, ftfy, deprecated, conllu, botocore, wikipedia-api, s3transfer, pytorch-revgrad, mpld3, bpemb, boto3, accelerate, transformer-smaller-training-vocab, flair\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.10\n",
      "    Uninstalling wcwidth-0.2.10:\n",
      "      Successfully uninstalled wcwidth-0.2.10\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "Successfully installed accelerate-0.24.1 boto3-1.29.6 botocore-1.32.6 bpemb-0.3.4 conllu-4.5.3 deprecated-1.2.14 flair-0.13.0 ftfy-6.1.3 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 mpld3-0.5.9 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.7.0 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.3.2 urllib3-1.26.18 wcwidth-0.2.12 wikipedia-api-0.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "urllib3",
         "wcwidth"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZw29qYFBh9y"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "The first step in processing a sentence with Flair is to convert it to a \"Sentence\" object, which is at first essentially a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSJb3x1iA1dj",
    "outputId": "1b3fd74c-1b3d-4a6b-cb74-a1fbe736aeeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (1.26.18)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.32.6 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
      "flair 0.13.0 requires urllib3<2.0.0,>=1.0.0, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-2.1.0\n",
      "Sentence[12]: \"Apple is looking at buying U.K. startup for $1 billion.\"\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade urllib3\n",
    "from flair.data import Sentence\n",
    "\n",
    "sent = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "sent_flair = Sentence(sent)\n",
    "print(sent_flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyD8Rpb5C076",
    "outputId": "a3a16076-9898-4eea-fa3b-cb6015ee73a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"Apple\"\n",
      "Token[1]: \"is\"\n",
      "Token[2]: \"looking\"\n",
      "Token[3]: \"at\"\n",
      "Token[4]: \"buying\"\n",
      "Token[5]: \"U.K.\"\n",
      "Token[6]: \"startup\"\n",
      "Token[7]: \"for\"\n",
      "Token[8]: \"$\"\n",
      "Token[9]: \"1\"\n",
      "Token[10]: \"billion\"\n",
      "Token[11]: \".\"\n"
     ]
    }
   ],
   "source": [
    "for token in sent_flair:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7JSJ9eJDJ6k"
   },
   "source": [
    "### POS tagging\n",
    "\n",
    "You can load a variety of sequence labelers. The default English POS tagger is just 'pos'.\n",
    "\n",
    "You just pass the Sentence object to the POS tagger and the Sentence is modified to include POS labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "847b37e7f2f54028aff2239c2a73d79f",
      "10c999685ea8407c92c67ee15c020f75",
      "8cef76db993b45fa8a7b5eb1ddc34139",
      "4007a5db98364e42bd737e66595dd5f2",
      "bdcba4579c0241258f56cb93f96ee602",
      "f1d56427a8194be1bdc92c371a2e1ce9",
      "61a1ba3dc1274c03a04798dc849463b2",
      "bd54f1631b574c8aaa50bf9a92123146",
      "3fcb59bb246a46c79b72d1d3d908f5c1",
      "ed3492332c4e431c8cb0734b51d17b5d",
      "4bddd7a756c343cd9a7fa1b47392c589"
     ]
    },
    "id": "U3dSgjmABBn1",
    "outputId": "93363896-1e80-4f03-8ed3-69034db79a86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847b37e7f2f54028aff2239c2a73d79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 04:14:36,212 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n",
      "Sentence[12]: \"Apple is looking at buying U.K. startup for $1 billion.\" → [\"Apple\"/NNP, \"is\"/VBZ, \"looking\"/VBG, \"at\"/IN, \"buying\"/VBG, \"U.K.\"/NNP, \"startup\"/NN, \"for\"/IN, \"$\"/$, \"1\"/CD, \"billion\"/CD, \".\"/.]\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "pos_tagger = SequenceTagger.load('pos')\n",
    "\n",
    "pos_tagger.predict(sent_flair)\n",
    "print(sent_flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_b0AageQEGHV",
    "outputId": "8dcfd563-60db-47d1-f41a-76e1f0c7a4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[12]: \"Apple is looking at buying U.K. startup for $1 billion.\" → [\"Apple\"/NNP, \"is\"/VBZ, \"looking\"/VBG, \"at\"/IN, \"buying\"/VBG, \"U.K.\"/NNP, \"startup\"/NN, \"for\"/IN, \"$\"/$, \"1\"/CD, \"billion\"/CD, \".\"/.]\n"
     ]
    }
   ],
   "source": [
    "print(sent_flair.to_tagged_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0xtLGOfFf7l"
   },
   "source": [
    "### Named entities\n",
    "\n",
    "Similarly there is an NER sequence tagger which adds labels to tokens indicated the beginning and end of \"spans\" that constitute named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140,
     "referenced_widgets": [
      "e5c3c29887ed48b1b6706a53594dc1f3",
      "64b7690bfa9549c18ee20ce086fec047",
      "75140c99a4ad43d4aca2e7b0158ad6c8",
      "8b8bf19ea8df47618ce24ea9f3e60116",
      "e64eb442b3c6451a82ce7615b5b811c8",
      "be2cb74b83c1494bb52ea4ce97e64faf",
      "bd7556ba82e943fb91c2453691e835a7",
      "a6324c8777cf4f0185d5b52c2315fd11",
      "48ebd2c46e884b2a815ca58c5f0c4afb",
      "c27f10c6fa67478ba6fc02a8fe300d15",
      "44386dd90e624ddd9fb42f0cbd2ac504"
     ]
    },
    "id": "D4ccGsIn_dX3",
    "outputId": "be5f2eb1-97eb-4726-c204-69489b32b3ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c3c29887ed48b1b6706a53594dc1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/432M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 04:15:23,635 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "Sentence[12]: \"Apple is looking at buying U.K. startup for $1 billion.\" → [\"Apple\"/NNP, \"Apple\"/ORG, \"is\"/VBZ, \"looking\"/VBG, \"at\"/IN, \"buying\"/VBG, \"U.K.\"/NNP, \"U.K.\"/LOC, \"startup\"/NN, \"for\"/IN, \"$\"/$, \"1\"/CD, \"billion\"/CD, \".\"/.]\n",
      "Span[0:1]: \"Apple\" → ORG (0.9992)\n",
      "Span[5:6]: \"U.K.\" → LOC (0.9955)\n"
     ]
    }
   ],
   "source": [
    "ner_tagger = SequenceTagger.load('ner')\n",
    "\n",
    "ner_tagger.predict(sent_flair)\n",
    "\n",
    "print(sent_flair.to_tagged_string())\n",
    "\n",
    "for entity in sent_flair.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1Qi5lJiGPeS"
   },
   "source": [
    "To analyze a full text, you need to turn it into a list of sentences. Within Flair, this is done by applying a \"Sentence splitter.\" Let's do this to our Trump speech and get the named entities again. This is a little slow (at least with the CPU runtime) -- takes about 1.5-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHeTziNYGg57",
    "outputId": "497a283b-f6c7-4a5a-db0c-d568d2e06207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span[1:3]: \"Justice Roberts\" → PER (0.9298)\n",
      "Span[5:6]: \"Carter\" → PER (0.9995)\n",
      "Span[8:9]: \"Clinton\" → PER (0.9962)\n",
      "Span[11:12]: \"Bush\" → PER (0.9935)\n",
      "Span[14:15]: \"Obama\" → PER (0.9986)\n",
      "Span[17:18]: \"Americans\" → MISC (0.995)\n",
      "Span[5:6]: \"America\" → LOC (0.9994)\n",
      "Span[8:9]: \"America\" → LOC (0.9927)\n",
      "Span[26:27]: \"Obama\" → PER (0.9997)\n",
      "Span[30:32]: \"Michelle Obama\" → PER (0.962)\n",
      "Span[27:29]: \"Washington DC\" → LOC (0.9485)\n",
      "Span[11:12]: \"Capital\" → LOC (0.5284)\n",
      "Span[0:1]: \"Washington\" → LOC (0.9732)\n",
      "Span[12:13]: \"America\" → LOC (0.9989)\n",
      "Span[4:8]: \"United States of America\" → LOC (0.8912)\n",
      "Span[0:1]: \"Americans\" → MISC (0.9925)\n",
      "Span[1:2]: \"American\" → MISC (0.9926)\n",
      "Span[14:15]: \"Americans\" → MISC (0.999)\n",
      "Span[13:14]: \"American\" → MISC (0.991)\n",
      "Span[55:56]: \"America\" → LOC (0.9994)\n",
      "Span[23:24]: \"American\" → MISC (0.996)\n",
      "Span[11:12]: \"America\" → LOC (0.9978)\n",
      "Span[14:15]: \"America\" → LOC (0.9966)\n",
      "Span[20:21]: \"American\" → MISC (0.9917)\n",
      "Span[23:24]: \"American\" → MISC (0.9946)\n",
      "Span[0:1]: \"America\" → LOC (0.943)\n",
      "Span[17:18]: \"American\" → MISC (0.9972)\n",
      "Span[20:21]: \"American\" → MISC (0.9971)\n",
      "Span[8:9]: \"American\" → MISC (0.9978)\n",
      "Span[11:12]: \"American\" → MISC (0.999)\n",
      "Span[17:18]: \"Islamic\" → MISC (0.9989)\n",
      "Span[29:30]: \"Earth\" → LOC (0.627)\n",
      "Span[13:17]: \"United States of America\" → LOC (0.8178)\n",
      "Span[1:2]: \"Bible\" → MISC (0.8378)\n",
      "Span[13:14]: \"God\" → PER (1.0)\n",
      "Span[1:2]: \"America\" → LOC (0.9997)\n",
      "Span[5:6]: \"America\" → LOC (0.9998)\n",
      "Span[25:26]: \"God\" → PER (0.9998)\n",
      "Span[1:2]: \"America\" → LOC (0.9996)\n",
      "Span[11:12]: \"America\" → LOC (0.9968)\n",
      "Span[49:51]: \"American Flag\" → MISC (0.8277)\n",
      "Span[11:12]: \"Detroit\" → LOC (0.9994)\n",
      "Span[17:18]: \"Nebraska\" → LOC (0.9997)\n",
      "Span[3:4]: \"Americans\" → MISC (0.9964)\n",
      "Span[13:14]: \"American\" → MISC (0.9967)\n",
      "Span[5:6]: \"America\" → LOC (0.9991)\n",
      "Span[3:4]: \"America\" → LOC (0.9976)\n",
      "Span[3:4]: \"America\" → LOC (0.9993)\n",
      "Span[3:4]: \"America\" → LOC (0.999)\n",
      "Span[9:10]: \"America\" → LOC (0.9984)\n",
      "Span[3:4]: \"God\" → PER (0.9994)\n",
      "Span[8:9]: \"God\" → PER (0.9988)\n",
      "Span[10:11]: \"America\" → LOC (0.9901)\n"
     ]
    }
   ],
   "source": [
    "#!pip install flair\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.splitter import SegtokSentenceSplitter\n",
    "\n",
    "# load the sentence splitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "\n",
    "trump_sentences = splitter.split(trump)\n",
    "\n",
    "ner_tagger.predict(trump_sentences)\n",
    "\n",
    "for sentence in trump_sentences:\n",
    "  for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65L0fC1vILcw"
   },
   "source": [
    "Flair has some unique-ish pretrained labelers available, such as a frame detector. It also has pretrained text classifiers for things like sentiment and offensive language (in German). The focus of this notebook is built-in sequence labeling, but I note that Flair's strengths become clearer in higher level tasks. It has extensive support for embeddings and applications like training new text classifiers."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0565ff4b466843d6814a60aefa3801b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d59c5c215b647a6aa75e7dbd2f1a50b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ee4fc62e516495c84408796e8c8ca53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "102fca374e4d49d0aafa74ccc8972abd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10c999685ea8407c92c67ee15c020f75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1d56427a8194be1bdc92c371a2e1ce9",
      "placeholder": "​",
      "style": "IPY_MODEL_61a1ba3dc1274c03a04798dc849463b2",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "2616895187194a5f8eb59fe28d6f91dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30a0ebb030d441cdb76e73bc2fdf2d33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d51779827ee4eba97ac6aa2e7a71b0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e906d99bdde40979763c10366f2d202": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fcb59bb246a46c79b72d1d3d908f5c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3fec37b560d043878ef0ffdfac4d84a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d51779827ee4eba97ac6aa2e7a71b0f",
      "placeholder": "​",
      "style": "IPY_MODEL_d8d41f8bed744ad59526baff61716604",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: "
     }
    },
    "4007a5db98364e42bd737e66595dd5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed3492332c4e431c8cb0734b51d17b5d",
      "placeholder": "​",
      "style": "IPY_MODEL_4bddd7a756c343cd9a7fa1b47392c589",
      "value": " 249M/249M [00:02&lt;00:00, 113MB/s]"
     }
    },
    "40f3704a3eba4026b8ebf6858848df7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d954d70670c546caa2d5c818b12098df",
      "placeholder": "​",
      "style": "IPY_MODEL_e027d71b5ef5492f8e93b732b22f9e8e",
      "value": " 367k/? [00:00&lt;00:00, 9.36MB/s]"
     }
    },
    "44386dd90e624ddd9fb42f0cbd2ac504": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48ebd2c46e884b2a815ca58c5f0c4afb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4bddd7a756c343cd9a7fa1b47392c589": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61a1ba3dc1274c03a04798dc849463b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "624de64a9eb94210bd49e91a71e880eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64b7690bfa9549c18ee20ce086fec047": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be2cb74b83c1494bb52ea4ce97e64faf",
      "placeholder": "​",
      "style": "IPY_MODEL_bd7556ba82e943fb91c2453691e835a7",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "64e5f4a97e4a48029bbc06745bd0bcde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "678647a588df4f778a1d0e7981ff7792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fec37b560d043878ef0ffdfac4d84a1",
       "IPY_MODEL_93ac252d21e44837a6ae7ebed50d58bc",
       "IPY_MODEL_40f3704a3eba4026b8ebf6858848df7e"
      ],
      "layout": "IPY_MODEL_d38bc9778a2c4c04af362c57bfdebf3d"
     }
    },
    "6cd4b27ff27244a992233ae17608ad7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2888beaf30f4b1e80d3123924dd25d9",
      "placeholder": "​",
      "style": "IPY_MODEL_30a0ebb030d441cdb76e73bc2fdf2d33",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: "
     }
    },
    "6d4d99fadc1e404baaad5b6c94cb91b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6562d25fdfa46209bb29c43a075b37d",
      "placeholder": "​",
      "style": "IPY_MODEL_acc5e1ca2da8429fbe1ea30100960b25",
      "value": " 367k/? [00:00&lt;00:00, 11.4MB/s]"
     }
    },
    "6e2c5a3a18ec4f21b3906afa6fc2d546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ee4fc62e516495c84408796e8c8ca53",
      "placeholder": "​",
      "style": "IPY_MODEL_624de64a9eb94210bd49e91a71e880eb",
      "value": " 595M/595M [00:14&lt;00:00, 47.8MB/s]"
     }
    },
    "6ef9dea7cabc4da7a4b445237e359032": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_900df244a85445b2b7e3e6648764a9f9",
       "IPY_MODEL_f83e103244c74f3f8898ae8311037cf4",
       "IPY_MODEL_6e2c5a3a18ec4f21b3906afa6fc2d546"
      ],
      "layout": "IPY_MODEL_2616895187194a5f8eb59fe28d6f91dc"
     }
    },
    "75140c99a4ad43d4aca2e7b0158ad6c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6324c8777cf4f0185d5b52c2315fd11",
      "max": 432176557,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_48ebd2c46e884b2a815ca58c5f0c4afb",
      "value": 432176557
     }
    },
    "847b37e7f2f54028aff2239c2a73d79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_10c999685ea8407c92c67ee15c020f75",
       "IPY_MODEL_8cef76db993b45fa8a7b5eb1ddc34139",
       "IPY_MODEL_4007a5db98364e42bd737e66595dd5f2"
      ],
      "layout": "IPY_MODEL_bdcba4579c0241258f56cb93f96ee602"
     }
    },
    "8b8bf19ea8df47618ce24ea9f3e60116": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c27f10c6fa67478ba6fc02a8fe300d15",
      "placeholder": "​",
      "style": "IPY_MODEL_44386dd90e624ddd9fb42f0cbd2ac504",
      "value": " 432M/432M [00:04&lt;00:00, 87.6MB/s]"
     }
    },
    "8bd11edc76ec4acabc2636bd820f9a31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d59c5c215b647a6aa75e7dbd2f1a50b",
      "max": 45744,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fcc370893d1444a8b16a57a154f54b6d",
      "value": 45744
     }
    },
    "8cef76db993b45fa8a7b5eb1ddc34139": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd54f1631b574c8aaa50bf9a92123146",
      "max": 249072763,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fcb59bb246a46c79b72d1d3d908f5c1",
      "value": 249072763
     }
    },
    "900df244a85445b2b7e3e6648764a9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e906d99bdde40979763c10366f2d202",
      "placeholder": "​",
      "style": "IPY_MODEL_0565ff4b466843d6814a60aefa3801b8",
      "value": "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/default.zip: 100%"
     }
    },
    "93ac252d21e44837a6ae7ebed50d58bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64e5f4a97e4a48029bbc06745bd0bcde",
      "max": 45744,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae88519205ee4aaab48b4080d2dc8a04",
      "value": 45744
     }
    },
    "a6324c8777cf4f0185d5b52c2315fd11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6562d25fdfa46209bb29c43a075b37d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acc5e1ca2da8429fbe1ea30100960b25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae88519205ee4aaab48b4080d2dc8a04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af6c399158d3473b9a310eda81555bd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b67e49bf262f42909a01662d2a3b5604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bd54f1631b574c8aaa50bf9a92123146": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd7556ba82e943fb91c2453691e835a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdcba4579c0241258f56cb93f96ee602": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be2cb74b83c1494bb52ea4ce97e64faf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c27f10c6fa67478ba6fc02a8fe300d15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2888beaf30f4b1e80d3123924dd25d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d38bc9778a2c4c04af362c57bfdebf3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8d41f8bed744ad59526baff61716604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d954d70670c546caa2d5c818b12098df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d95b11f49bef4d12b2eb80a47a940c5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6cd4b27ff27244a992233ae17608ad7e",
       "IPY_MODEL_8bd11edc76ec4acabc2636bd820f9a31",
       "IPY_MODEL_6d4d99fadc1e404baaad5b6c94cb91b8"
      ],
      "layout": "IPY_MODEL_102fca374e4d49d0aafa74ccc8972abd"
     }
    },
    "e027d71b5ef5492f8e93b732b22f9e8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5c3c29887ed48b1b6706a53594dc1f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64b7690bfa9549c18ee20ce086fec047",
       "IPY_MODEL_75140c99a4ad43d4aca2e7b0158ad6c8",
       "IPY_MODEL_8b8bf19ea8df47618ce24ea9f3e60116"
      ],
      "layout": "IPY_MODEL_e64eb442b3c6451a82ce7615b5b811c8"
     }
    },
    "e64eb442b3c6451a82ce7615b5b811c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed3492332c4e431c8cb0734b51d17b5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1d56427a8194be1bdc92c371a2e1ce9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f83e103244c74f3f8898ae8311037cf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af6c399158d3473b9a310eda81555bd5",
      "max": 594884295,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b67e49bf262f42909a01662d2a3b5604",
      "value": 594884295
     }
    },
    "fcc370893d1444a8b16a57a154f54b6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
