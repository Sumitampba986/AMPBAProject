{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOURrT8QqsOy"
   },
   "source": [
    "<a href=\"http://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project06%20-%20Text%20Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy7_8EYHqsO0"
   },
   "source": [
    "# Document Summarization\n",
    "\n",
    "The idea of document summarization is a\n",
    "bit different from keyphrase extraction or topic modeling. In this case, the end result\n",
    "is still in the form of some document, but with a few sentences based on the length we\n",
    "might want the summary to be. This is similar to an abstract or an executive summary\n",
    "in a research paper. The main objective of automated document summarization is\n",
    "to perform this summarization without involving human input, except for running\n",
    "computer programs. Mathematical and statistical models help in building and\n",
    "automating the task of summarizing documents by observing their content and context.\n",
    "\n",
    "There are two broad approaches to document summarization using automated\n",
    "techniques. They are described as follows:\n",
    "- __Extraction-based techniques:__ These methods use mathematical\n",
    "and statistical concepts like SVD to extract some key subset of the\n",
    "content from the original document such that this subset of content\n",
    "contains the core information and acts as the focal point of the entire\n",
    "document. This content can be words, phrases, or even sentences.\n",
    "The end result from this approach is a short executive summary of a\n",
    "couple of lines extracted from the original document. No new content\n",
    "is generated in this technique, hence the name extraction-based.\n",
    "- __Abstraction-based techniques:__ These methods are more complex\n",
    "and sophisticated. They leverage language semantics to create\n",
    "representations and use natural language generation (NLG)\n",
    "techniques where the machine uses knowledge bases and semantic\n",
    "representations to generate text on its own and create summaries\n",
    "just like a human would write them. Thanks to deep learning, we can\n",
    "implement these techniques easily but they require a lot of data and\n",
    "compute.\n",
    "\n",
    "We will cover extraction based methods here due to constraints of needed a lot of data + compute for abstraction based methods. But you can leverage the seq2seq models you learnt in language translation on an appropriate dataset to build deep learning based abstractive summarizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaeNhzvtqsO1"
   },
   "source": [
    "# Install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TKqj2jxDWjN",
    "outputId": "8a6df369-de73-4933-9565-7773e67413a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTPNzTzjqsO3"
   },
   "source": [
    "# Get Text Document\n",
    "\n",
    "We use the description of a very popular role-playing game (RPG) Skyrim from\n",
    "Bethesda Softworks for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sSrjtPxCZeq"
   },
   "outputs": [],
   "source": [
    "DOCUMENT = \"\"\"\n",
    "The Elder Scrolls V: Skyrim is an action role-playing video game developed by Bethesda Game Studios\n",
    "and published by Bethesda Softworks. It is the fifth main installment in The Elder Scrolls series,\n",
    "following The Elder Scrolls IV: Oblivion.\n",
    "The game's main story revolves around the player character's quest to defeat Alduin the World-Eater,\n",
    "a dragon who is prophesied to destroy the world. The game is set 200 years after the events of Oblivion\n",
    "and takes place in the fictional province of Skyrim. Over the course of the game, the player completes\n",
    "quests and develops the character by improving skills. The game continues the open-world tradition of\n",
    "its predecessors by allowing the player to travel anywhere in the game world at any time, and to ignore\n",
    "or postpone the main storyline indefinitely.\n",
    "The team opted for a unique and more diverse open world than Oblivion's Imperial Province of Cyrodiil,\n",
    "which game director and executive producer Todd Howard considered less interesting by comparison.\n",
    "The game was released to critical acclaim, with reviewers particularly mentioning the character advancement\n",
    "and setting, and is considered to be one of the greatest video games of all time.\n",
    "\n",
    "\n",
    "The Elder Scrolls V: Skyrim is an action role-playing game, playable from either a first or\n",
    "third-person perspective. The player may freely roam over the land of Skyrim which is an open world\n",
    "environment consisting of wilderness expanses, dungeons, cities, towns, fortresses, and villages.\n",
    "Players may navigate the game world more quickly by riding horses or by utilizing a fast-travel system\n",
    "which allows them to warp to previously discovered locations. The game's main quest can be completed or\n",
    "ignored at the player's preference after the first stage of the quest is finished. However, some quests\n",
    "rely on the main storyline being at least partially completed. Non-player characters (NPCs) populate the\n",
    "world and can be interacted with in a number of ways: the player may engage them in conversation,\n",
    "marry an eligible NPC, kill them or engage in a nonlethal \"brawl\". The player may\n",
    "choose to join factions which are organized groups of NPCs — for example, the Dark Brotherhood, a band\n",
    "of assassins. Each of the factions has an associated quest path to progress through. Each city and town\n",
    "in the game world has jobs that the player can engage in, such as farming.\n",
    "\n",
    "Players have the option to develop their character. At the beginning of the game, players create\n",
    "their character by selecting their sex and choosing between one of several races including humans,\n",
    "orcs, elves, and anthropomorphic cat or lizard-like creatures and then customizing their character's\n",
    "appearance. Over the course of the game, players improve their character's skills which are numerical\n",
    "representations of their ability in certain areas. There are eighteen skills divided evenly among the\n",
    "three schools of combat, magic, and stealth. When players have trained skills enough to meet the\n",
    "required experience, their character levels up. Health is depleted primarily when the player\n",
    "takes damage and the loss of all health results in death. Magicka is depleted by the use of spells,\n",
    "certain poisons and by being struck by lightning-based attacks. Stamina determines the player's\n",
    "effectiveness in combat and is depleted by sprinting, performing heavy \"power attacks\"\n",
    "and being struck by frost-based attacks. Skyrim is the first entry in The Elder Scrolls to\n",
    "include dragons in the game's wilderness. Like other creatures, dragons are generated randomly in\n",
    "the world and will engage in combat with NPCs, creatures and the player. Some dragons may attack\n",
    "cities and towns when in their proximity. The player character can absorb the souls of dragons\n",
    "in order to use powerful spells called \"dragon shouts\" or \"Thu'um\". A regeneration\n",
    "period limits the player's use of shouts in gameplay.\n",
    "\n",
    "Skyrim is set around 200 years after the events of The Elder Scrolls IV: Oblivion, although it is\n",
    "not a direct sequel. The game takes place in Skyrim, a province of the Empire on the continent of\n",
    "Tamriel, amid a civil war between two factions: the Stormcloaks, led by Ulfric Stormcloak, and the\n",
    "Imperial Legion, led by General Tullius. The player character is a Dragonborn, a mortal born with\n",
    "the soul and power of a dragon. Alduin, a large black dragon who returns to the land after being\n",
    "lost in time, serves as the game's primary antagonist. Alduin is the first dragon created by Akatosh,\n",
    "one of the series' gods, and is prophesied to destroy and consume the world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Sp4ZBphDBTy"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)\n",
    "DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
    "DOCUMENT = DOCUMENT.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJwlw5hpqsO4"
   },
   "source": [
    "# Summarization with Gensim\n",
    "\n",
    "Let’s look at an implementation of document summarization by leveraging Gensim’s\n",
    "summarization module. It is pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJfaklbaDGm3",
    "outputId": "e6903aa8-f99e-4cb8-d329-f1251163923f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.8.3\n",
      "  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.11.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (6.4.0)\n",
      "Building wheels for collected packages: gensim\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
      "Failed to build gensim\n",
      "\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6fb1bb66a2b1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim==3.8.3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDOCUMENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.8.3\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "print(summarize(DOCUMENT, ratio=0.2, split=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H83_YvADI2j"
   },
   "outputs": [],
   "source": [
    "print(summarize(DOCUMENT, word_count=75, split=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4ObNmxqDL1N"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(DOCUMENT)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48UIEsb8qsO5"
   },
   "source": [
    "This summarization implementation from Gensim is based on a variation of\n",
    "a popular algorithm called TextRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16DEix50qsO5"
   },
   "source": [
    "# Basic Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWTCv0YQDPYR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_sentences = normalize_corpus(sentences)\n",
    "norm_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MyqGHD2qsO5"
   },
   "source": [
    "# Text Representation with Feature Engineering\n",
    "\n",
    "We will be vectorizing our normalized sentences using the TF-IDF feature engineering\n",
    "scheme. We keep things simple and don’t filter out any words based on document\n",
    "frequency. But feel free to try that out and maybe even leverage n-grams as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aGDxNQyDpiY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "dt_matrix = tv.fit_transform(norm_sentences)\n",
    "dt_matrix = dt_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "td_matrix = dt_matrix.T\n",
    "print(td_matrix.shape)\n",
    "pd.DataFrame(np.round(td_matrix, 2), index=vocab).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IJPWUGwqsO6"
   },
   "source": [
    "# Latent Semantic Analysis\n",
    "\n",
    "Here, we summarize our game description by utilizing document sentences. The\n",
    "terms in each sentence of the document have been extracted to form the term-document matrix, which we observed in the previous cell.\n",
    "\n",
    "We apply low-rank Singular Value Decomposition to this matrix. The core principle behind Latent Semantic Analysis (LSA) is that in any document, there exists a latent structure among terms that are related\n",
    "contextually and hence should also be correlated in the same singular space.\n",
    "\n",
    "The main idea in our implementation is to use SVD (recall M = USVT) so that U\n",
    "and V are the orthogonal matrices and S is the diagonal matrix, which can also be\n",
    "represented as a vector of the singular values.\n",
    "\n",
    "The original matrix can be represented as\n",
    "a term-document matrix where the rows are terms and each column is a document, i.e.,\n",
    "a sentence from our document in this case. The values can be any type of weighting like\n",
    "Bag of Words model-based frequencies, TF-IDFs, or binary occurrences.\n",
    "\n",
    "![](https://i.imgur.com/YtopNr3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLmDllaUDxIV"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnkxRcQ8D08f"
   },
   "outputs": [],
   "source": [
    "num_sentences = 8\n",
    "num_topics = 3\n",
    "\n",
    "u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)\n",
    "print(u.shape, s.shape, vt.shape)\n",
    "term_topic_mat, singular_values, topic_document_mat = u, s, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Bo45Qj3D5SD"
   },
   "outputs": [],
   "source": [
    "# remove singular values below threshold\n",
    "sv_threshold = 0.5\n",
    "min_sigma_value = max(singular_values) * sv_threshold\n",
    "singular_values[singular_values < min_sigma_value] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ItEFwCXD8qk"
   },
   "outputs": [],
   "source": [
    "salience_scores = np.sqrt(np.dot(np.square(singular_values),\n",
    "                                 np.square(topic_document_mat)))\n",
    "salience_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xn9EjCGpFRCH"
   },
   "outputs": [],
   "source": [
    "top_sentence_indices = (-salience_scores).argsort()[:num_sentences]\n",
    "top_sentence_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLjP9KgbFUNi"
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(np.array(sentences)[top_sentence_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiswV1hYqsO6"
   },
   "source": [
    "# TextRank\n",
    "\n",
    "The TextRank summarization algorithm internally uses the popular PageRank\n",
    "algorithm, which is used by Google for ranking websites and pages. This is used by the\n",
    "Google search engine when providing relevant web pages based on search queries. To\n",
    "understand TextRank better, we need to understand some of the concepts surrounding\n",
    "PageRank. The core algorithm in PageRank is a graph-based scoring or ranking\n",
    "algorithm, where pages are scored or ranked based on their importance.\n",
    "\n",
    "Websites and\n",
    "pages contain further links embedded in them which link to more pages having more\n",
    "links and this continues across the Internet. This can be represented as a graph-based\n",
    "model where vertices indicate the web pages and edges indicate links among them. This\n",
    "can be used to form a voting or recommendation system such so when one vertex links\n",
    "to another one in the graph it is basically casting a vote.\n",
    "\n",
    "Vertex importance is decided\n",
    "not only on the number of votes or edges but also the importance of the vertices that are\n",
    "connected to it and their importance.\n",
    "\n",
    "![](https://i.imgur.com/fMyJjUN.png)\n",
    "\n",
    "We can see that vertex denoting Page C has a higher score than\n",
    "Page E even if it has fewer edges compared to Page E, because Page B is an important\n",
    "page connected to Page C.\n",
    "\n",
    "For textrank we will follow a similar process leveraging pagerank\n",
    "\n",
    "![](https://i.imgur.com/kkrzeq7.png)\n",
    "\n",
    "- Tokenize and extract sentences from the document to be\n",
    "summarized.\n",
    "- Decide on the number of sentences, k, that we want in the final\n",
    "summary\n",
    "- Build a document-term feature matrix using weights like TF-IDF\n",
    "or Bag of Words.\n",
    "- Compute a document similarity matrix by multiplying the matrix\n",
    "by its transpose.\n",
    "- Use these documents (sentences in our case) as the vertices and\n",
    "the similarities between each pair of documents as the weight\n",
    "or score coefficient we talked about earlier and feed them to the\n",
    "PageRank algorithm.\n",
    "- Get the score for each sentence.\n",
    "- Rank the sentences based on score and return the top k sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmtJSSTyqsO7"
   },
   "source": [
    "# Build Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwy27oUUFVwC"
   },
   "outputs": [],
   "source": [
    "similarity_matrix = np.matmul(dt_matrix, dt_matrix.T)\n",
    "print(similarity_matrix.shape)\n",
    "np.round(similarity_matrix, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBhW-8xqqsO7"
   },
   "source": [
    "# Build Similarity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XKLLMxVFZFu"
   },
   "outputs": [],
   "source": [
    "import networkx\n",
    "\n",
    "similarity_graph = networkx.from_numpy_array(similarity_matrix)\n",
    "similarity_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8hGp-gQFbOs"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "networkx.draw_networkx(similarity_graph, node_color='lime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GLJr_y-qsO7"
   },
   "source": [
    "# Get Sentence Importance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9M9Wz2lFdej"
   },
   "outputs": [],
   "source": [
    "scores = networkx.pagerank(similarity_graph)\n",
    "ranked_sentences = sorted(((score, index) for index, score\n",
    "                                            in scores.items()),\n",
    "                          reverse=True)\n",
    "ranked_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62NO_yc0Ff5C"
   },
   "outputs": [],
   "source": [
    "top_sentence_indices = [ranked_sentences[index][1]\n",
    "                        for index in range(num_sentences)]\n",
    "top_sentence_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idmgKnTuFiyg"
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(np.array(sentences)[top_sentence_indices]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
